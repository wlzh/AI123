

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>微调BaiChuan13B来做命名实体识别 作者： AINLP 来源： AINLP 传统上，一般把NLP的研究领域大致分为自然语言理解（NLU）和自然语言生成（NLG）两种。 NLU侧重于如何理解文本，包括文本分类、命名实体识别、指代消歧、句法分析、机器阅读理解等； NLG则侧重于理解文本后如何生成自然文本，包  | AI123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AI123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-LoCoq3KOzQ" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2634092855285462"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://ai123.869hr.uk/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://ai123.869hr.uk/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AI123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">微调BaiChuan13B来做命名实体识别</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://ai123.869hr.uk/about>AI123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              July 24, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7YIY2pzwPrVHB8pVltLz8VkHHpFTNBVFKKIUzLtmj0dxKVjkYReeQKA/640?wx_fmt=png" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： AINLP  来源： <a href="https://mp.weixin.qq.com/s/Px4h2r3VIAFI5vfjXxROxg">AINLP</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJuK8UUBxdZXj1c20hUg374YPgXibgDGytAy87YxvVk4WCRFWrdKJPshStrlPJp4vGEGUQodxt7ibOw/640?wx_fmt=jpeg" alt=""></p>
<p>传统上，一般把NLP的研究领域大致分为自然语言理解（NLU）和自然语言生成（NLG）两种。</p>
<p>NLU侧重于如何理解文本，包括文本分类、命名实体识别、指代消歧、句法分析、机器阅读理解等；</p>
<p>NLG则侧重于理解文本后如何生成自然文本，包括自动摘要、机器翻译、问答系统、对话机器人等。</p>
<p><strong>但是以ChatGPT为代表的大模型出来后，这些传统的NLP的细分研究领域基本可以说都失去了独立研究的价值。</strong></p>
<p>为什么呢？因为大模型可以用统一的范式通通将它们搞定，并且效果非常出众。</p>
<p>在之前的例子中，我们演示了使用QLoRA算法来对BaiChuan-13B实施微调以处理最简单的文本分类任务。</p>
<p><a href="http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650438167&amp;idx=2&amp;sn=0f7fd3d2ca8260a7d37ef8728b04a0ed&amp;chksm=becdfc4d89ba755b6f64983b0c1c32bb0fcaf5c06fa8e99a88dcfdf9d4d2f9633e37e325c88e&amp;scene=21#wechat_redirect">Baichuan-13B 保姆级微调范例</a></p>
<p>在外卖评论数据集上，微调后测试集acc由0.8925提升到0.9015约提升了1个百分点。</p>
<p>在本例中，我们使用几乎相同的流程和方法来微调BaiChuan-13B以更好地处理命名实体识别任务。</p>
<p><strong>实验结果显示，在NER任务上经过微调，我们的f1-score取得了不可忽略的提升（</strong> <strong>0.4313—&gt;0.8768</strong> <strong>）。</strong></p>
<p>注：跑完本流程需要至少32G的GPU，需要约2个小时的训练时间。</p>
<p>在我们正式开始之前，请允许我用简短的话给没有NLP基础知识的小伙伴讲解一下什么是命名实体识别。</p>
<p>命名实体识别NER任务是NLP的一个常见基础任务，</p>
<p>它是Named Entity Recognization的简称。</p>
<p>简单地说，就是识别一个句子中的各种 名称实体，诸如：人名，地名，机构 等。</p>
<p>例如对于下面这句话：</p>
<pre><code>小明对小红说:&quot;你听说过安利吗？&quot;  
</code></pre>
<p>其命名实体可以抽取表示如下：</p>
<pre><code>{&quot;人名&quot;: [&quot;小明&quot;,&quot;小红&quot;], &quot;组织&quot;: [&quot;安利&quot;]}  
</code></pre>
<h4 id="预训练模型">〇，预训练模型</h4>
<p>我们需要从 <a href="https://huggingface.co/baichuan-inc/Baichuan-13B-Chat">https://huggingface.co/baichuan-inc/Baichuan-13B-Chat</a> 下载baichuan-13b-chat的模型。</p>
<p>国内可能速度会比较慢，总共有25个G左右，网速不太好的话，大概可能需要两到三个小时。</p>
<p>如果网络不稳定，也可以手动从这个页面一个一个下载全部文件然后放置到 一个文件夹中例如 &lsquo;baichuan-13b&rsquo; 以便读取。</p>
<pre><code>import warnings  
warnings.filterwarnings('ignore')  



import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, AutoModel, BitsAndBytesConfig  
from transformers.generation.utils import GenerationConfig  
import torch.nn as nn  
  
  
#使用QLoRA引入的 NF4量化数据类型以节约显存  
model_name_or_path ='../baichuan-13b' #远程 'baichuan-inc/Baichuan-13B-Chat'  
  
bnb_config=BitsAndBytesConfig(  
            load_in_4bit=True,  
            bnb_4bit_compute_dtype=torch.float16,  
            bnb_4bit_use_double_quant=True,  
            bnb_4bit_quant_type=&quot;nf4&quot;,  
            llm_int8_threshold=6.0,  
            llm_int8_has_fp16_weight=False,  
        )  
  
tokenizer = AutoTokenizer.from_pretrained(  
   model_name_or_path, trust_remote_code=True)  
  
model = AutoModelForCausalLM.from_pretrained(model_name_or_path,  
                quantization_config=bnb_config,  
                trust_remote_code=True)   
  
model.generation_config = GenerationConfig.from_pretrained(model_name_or_path)  
  



from IPython.display import clear_output   
messages = []  
messages.append({&quot;role&quot;: &quot;user&quot;,  
                 &quot;content&quot;: &quot;世界上第二高的山峰是哪座?&quot;})  
response = model.chat(tokenizer,messages=messages,stream=True)  
for res in response:  
    print(res)  
    clear_output(wait=True)  
      
      
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7lych3iaLHHENxW8SQs6WEku35ddLO7cF2p3yO3AbDomuaMtf2ianWnpg/640?wx_fmt=png" alt=""></p>
<p>下面我们设计一个7-shot-prompt方法，测试一下BaiChuan13b的实体抽取能力。</p>
<pre><code>prefix = '''命名实体识别：抽取文本中的 人名，地点，组织 这三类命名实体，并按照json格式返回结果。  
  
下面是一些范例：  
  
小明对小红说:&quot;你听说过安利吗？&quot; -&gt; {&quot;人名&quot;: [&quot;小明&quot;,&quot;小红&quot;], &quot;组织&quot;: [&quot;安利&quot;]}  
现在，每年有几十万中国人到美国访问，几千名中国留学生到美国就学。 -&gt; {&quot;地点&quot;: [&quot;中国&quot;, &quot;美国&quot;]}  
中国是联合国安理会常任理事国之一。 -&gt; {&quot;地点&quot;: [&quot;中国&quot;], &quot;组织&quot;: [&quot;联合国&quot;]}  
  
请对下述文本进行实体抽取，返回json格式。  
  
'''  
  
def get_prompt(text):  
    return prefix+text+' -&gt; '  
  
def get_message(prompt,response):  
    return [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f'{prompt} -&gt; '},  
            {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response}]  
  



messages  = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: get_prompt(&quot;一些摩洛哥球迷已按捺不住，在看台上欢呼雀跃&quot;)}]  
response = model.chat(tokenizer, messages)  
print(response)  
  



{&quot;地点&quot;:[&quot;摩洛哥&quot;], &quot;组织&quot;:[]}  



messages = messages+[{&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;{'地点': ['摩洛哥']}&quot;}]  
messages.extend(get_message(&quot;这次轮到北京国安队，不知会不会再步后尘？&quot;,&quot;{'组织': ['北京国安队']}&quot;))  
messages.extend(get_message(&quot;革命党人孙中山在澳门成立同盟会分会&quot;,&quot;{'人名': ['孙中山'], '地名': ['澳门'], '组织': ['同盟会']}&quot;))  
messages.extend(get_message(&quot;我曾在安徽芜湖市和上海浦东打工。&quot;,&quot;{'地点': ['安徽芜湖市', '上海浦东']}&quot;))  
display(messages)  
  



  



![](https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7ichX42CAGJr3PiclNsIyjM5CosYkvFgNEY97FL8AUFxhNjf8uw8ChW7g/640?wx_fmt=png)

  

  



def predict(text,temperature=0.01):  
    model.generation_config.temperature=temperature  
    response = model.chat(tokenizer,   
                          messages = messages+[{'role':'user','content':f'{text} -&gt; '}])  
    return response  
  



predict('杜甫是李白的粉丝。')   



&quot;{'人名': ['杜甫', '李白']}&quot;  
</code></pre>
<p>我们拿一个开源的中文NER数据集来测试一下未经微调，仅仅使用7-shot-prompt的预训练模型的效果。</p>
<pre><code>from sklearn.model_selection import train_test_split  
import pandas as pd   
  
df = pd.read_pickle('dfner_13k.pkl')  
dfdata,dftest = train_test_split(df,test_size=300,random_state=42)  
dftrain,dfval = train_test_split(dfdata,test_size=200,random_state=42)  



preds = ['' for x in dftest['target']]  
for i in tqdm(range(len(preds))):  
    preds[i] = predict(dftest['text'].iloc[i])  
      



def toset(s):  
    try:  
        dic = eval(str(s))  
        res = []  
        for k,v in dic.items():  
            for x in v:  
                if x:  
                    res.append((k,x))  
        return set(res)  
    except Exception as err:  
        print(err)  
        return set()  



dftest['pred'] = [toset(x) for x in preds]  
dftest['gt'] = [toset(x) for x in dftest['target']]  
dftest['tp_cnt'] = [len(pred&amp;gt) for pred,gt in zip(dftest['pred'],dftest['gt'])]  
dftest['pred_cnt'] = [len(x) for x in dftest['pred']]  
dftest['gt_cnt'] = [len(x) for x in dftest['gt']]  
  
precision = sum(dftest['tp_cnt'])/sum(dftest['pred_cnt'])  
print('precision = '+str(precision))  
  
recall = sum(dftest['tp_cnt'])/sum(dftest['gt_cnt'])  
print('recall = '+str(recall))  
  
f1 = 2*precision*recall/(precision+recall)  
print('f1_score = '+str(f1))  
  



precision = 0.4316109422492401  
recall = 0.45151033386327505  
f1_score = 0.44133644133644134  
</code></pre>
<p><strong>微调前 f1_score为 0.44.</strong></p>
<h4 id="一准备数据">一，准备数据</h4>
<p>我们仿照百川模型的 model._build_chat_input
方法来进行token编码，同时把需要学习的内容添加label.</p>
<h4 id="1token编码">1，token编码</h4>
<pre><code>import torch   
  
#将messages编码成 token, 同时返回labels  
#注意baichuan-13b通过插入tokenizer.user_token_id和tokenizer.assistant_token_id 来区分用户和机器人会话内容  
  
# reference@ model._build_chat_input?  
def build_chat_input(messages, model=model,  
                     tokenizer=tokenizer,   
                     max_new_tokens: int=0):  
    max_new_tokens = max_new_tokens or model.generation_config.max_new_tokens  
    max_input_tokens = model.config.model_max_length - max_new_tokens  
    max_input_tokens = max(model.config.model_max_length // 2, max_input_tokens)  
      
    total_input, round_input, total_label, round_label = [], [], [], []  
      
    for i, message in enumerate(messages[::-1]):  
        content_tokens = tokenizer.encode(message['content'])  
        if message['role'] == 'user':  
            round_input = [model.generation_config.user_token_id] + content_tokens + round_input  
            round_label = [-100]+[-100 for _ in content_tokens]+ round_label  
              
            if total_input and len(total_input) + len(round_input) &gt; max_input_tokens:  
                break  
            else:  
                total_input = round_input + total_input  
                total_label = round_label + total_label  
                if len(total_input) &gt;= max_input_tokens:  
                    break  
                else:  
                    round_input = []  
                    round_label = []  
                      
        elif message['role'] == 'assistant':  
            round_input = [  
                model.generation_config.assistant_token_id  
            ] + content_tokens + [  
                model.generation_config.eos_token_id  
            ] + round_input  
              
            if i==0: #仅对最后一轮的target进行学习  
                round_label = [  
                    -100  
                ] + content_tokens + [  
                    model.generation_config.eos_token_id  
                ]+ round_label  
            else:  
                round_label = [  
                    -100  
                ] + [-100 for _ in content_tokens] + [  
                    -100  
                ]+ round_label  
                  
        else:  
            raise ValueError(f&quot;message role not supported yet: {message['role']}&quot;)  
              
    total_input = total_input[-max_input_tokens:]  # truncate left  
    total_label = total_label[-max_input_tokens:]  
      
    total_input.append(model.generation_config.assistant_token_id)  
    total_label.append(-100)  
      
    return total_input,total_label  
</code></pre>
<h4 id="2做数据集">2，做数据集</h4>
<pre><code>from torch.utils.data import Dataset,DataLoader   
from copy import deepcopy  
class MyDataset(Dataset):  
    def __init__(self,df,  
                 messages  
                ):  
        self.df = df   
        self.messages = messages  
          
    def __len__(self):  
        return len(self.df)  
          
    def get_samples(self,index):  
        samples = []  
        d = dict(self.df.iloc[index])  
        samples.append(d)  
        return samples  
      
    def get_messages(self,index):  
        samples = self.get_samples(index)  
        messages = deepcopy(self.messages)  
        for i,d in enumerate(samples):  
  
            messages.append({'role':'user','content':d['text']+' -&gt; '})  
            messages.append({'role':'assistant','content':str(d['target'])})  
        return messages  
          
    def __getitem__(self,index):  
        messages = self.get_messages(index)  
        input_ids, labels = build_chat_input(messages)  
        return {'input_ids':input_ids,'labels':labels}  
  
    def show_sample(self,index):  
        samples = self.get_samples(index)  
        print(samples)  
      
      



ds_train = MyDataset(dftrain,messages)  
ds_val = MyDataset(dfval,messages)  
</code></pre>
<h4 id="3创建管道">3，创建管道</h4>
<pre><code>def data_collator(examples: list):  
    len_ids = [len(example[&quot;input_ids&quot;]) for example in examples]  
    longest = max(len_ids) #之后按照batch中最长的input_ids进行padding  
      
    input_ids = []  
    labels_list = []  
      
    for length, example in sorted(zip(len_ids, examples), key=lambda x: -x[0]):  
        ids = example[&quot;input_ids&quot;]  
        labs = example[&quot;labels&quot;]  
          
        ids = ids + [tokenizer.pad_token_id] * (longest - length)  
        labs = labs + [-100] * (longest - length)  
          
        input_ids.append(torch.LongTensor(ids))  
        labels_list.append(torch.LongTensor(labs))  
            
    input_ids = torch.stack(input_ids)  
    labels = torch.stack(labels_list)  
    return {  
        &quot;input_ids&quot;: input_ids,  
        &quot;labels&quot;: labels,  
    }  
  



import torch   
dl_train = torch.utils.data.DataLoader(ds_train,num_workers=2,batch_size=1,  
                                       pin_memory=True,shuffle=True,  
                                       collate_fn = data_collator)  
  
dl_val = torch.utils.data.DataLoader(ds_val,num_workers=2,batch_size=1,  
                                    pin_memory=True,shuffle=False,  
                                     collate_fn = data_collator)  
  



for batch in dl_train:  
    break   



#试跑一个batch  
out = model(**batch)  
out.loss   



#采样300个batch作为一个epoch，便于较快验证  
dl_train.size = 300  
</code></pre>
<h4 id="二定义模型">二，定义模型</h4>
<p>下面我们将使用QLoRA(实际上用的是量化的AdaLoRA）算法来微调Baichuan-13b模型。</p>
<pre><code>from peft import get_peft_config, get_peft_model, TaskType  
model.supports_gradient_checkpointing = True  #  
model.gradient_checkpointing_enable()  
model.enable_input_require_grads()  
  
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!  
  



import bitsandbytes as bnb   
def find_all_linear_names(model):  
    &quot;&quot;&quot;  
    找出所有全连接层，为所有全连接添加adapter  
    &quot;&quot;&quot;  
    cls = bnb.nn.Linear4bit  
    lora_module_names = set()  
    for name, module in model.named_modules():  
        if isinstance(module, cls):  
            names = name.split('.')  
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])  
  
    if 'lm_head' in lora_module_names:  # needed for 16-bit  
        lora_module_names.remove('lm_head')  
    return list(lora_module_names)  
  



from peft import prepare_model_for_kbit_training   
model = prepare_model_for_kbit_training(model)  
  



lora_modules = find_all_linear_names(model)  
print(lora_modules)   
  



['down_proj', 'gate_proj', 'W_pack', 'o_proj', 'up_proj']  



from peft import AdaLoraConfig  
peft_config = AdaLoraConfig(  
    task_type=TaskType.CAUSAL_LM, inference_mode=False,  
    r=16,  
    lora_alpha=16, lora_dropout=0.05,  
    target_modules= lora_modules  
)  
  
peft_model = get_peft_model(model, peft_config)  
  
peft_model.is_parallelizable = True  
peft_model.model_parallel = True  
peft_model.print_trainable_parameters()  
  



trainable params: 41,843,040 || all params: 7,002,181,160 || trainable%: 0.5975715144165165  



out = peft_model.forward(**batch)  
out[0]  
</code></pre>
<h4 id="三训练模型">三，训练模型</h4>
<pre><code>from torchkeras import KerasModel   
from accelerate import Accelerator   
  
class StepRunner:  
    def __init__(self, net, loss_fn, accelerator=None, stage = &quot;train&quot;, metrics_dict = None,   
                 optimizer = None, lr_scheduler = None  
                 ):  
        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage  
        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler  
        self.accelerator = accelerator if accelerator is not None else Accelerator()   
        if self.stage=='train':  
            self.net.train()   
        else:  
            self.net.eval()  
      
    def __call__(self, batch):  
          
        #loss  
        with self.accelerator.autocast():  
            loss = self.net.forward(**batch)[0]  
  
        #backward()  
        if self.optimizer is not None and self.stage==&quot;train&quot;:  
            self.accelerator.backward(loss)  
            if self.accelerator.sync_gradients:  
                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)  
            self.optimizer.step()  
            if self.lr_scheduler is not None:  
                self.lr_scheduler.step()  
            self.optimizer.zero_grad()  
              
        all_loss = self.accelerator.gather(loss).sum()  
          
        #losses (or plain metrics that can be averaged)  
        step_losses = {self.stage+&quot;_loss&quot;:all_loss.item()}  
          
        #metrics (stateful metrics)  
        step_metrics = {}  
          
        if self.stage==&quot;train&quot;:  
            if self.optimizer is not None:  
                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']  
            else:  
                step_metrics['lr'] = 0.0  
        return step_losses,step_metrics  
      
KerasModel.StepRunner = StepRunner   
  
#仅仅保存QLora可训练参数  
def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):  
    unwrap_net = accelerator.unwrap_model(self.net)  
    unwrap_net.save_pretrained(ckpt_path)  
      
def load_ckpt(self, ckpt_path='checkpoint'):  
    import os  
    self.net.load_state_dict(  
        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)  
    self.from_scratch = False  
      
KerasModel.save_ckpt = save_ckpt   
KerasModel.load_ckpt = load_ckpt   
  



optimizer = bnb.optim.adamw.AdamW(peft_model.parameters(),  
                                  lr=6e-05,is_paged=True)  #'paged_adamw'  
keras_model = KerasModel(peft_model,loss_fn =None,  
        optimizer=optimizer)   
ckpt_path = 'baichuan13b_ner'  
  
  



# keras_model.load_ckpt(ckpt_path) #支持加载微调后的权重继续训练(断点续训)  
keras_model.fit(train_data = dl_train,  
                val_data = dl_val,  
                epochs=100,patience=10,  
                monitor='val_loss',mode='min',  
                ckpt_path = ckpt_path  
               )  
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7hnebic6hibcOsbIofLQ7jU3iasZ9uWnBJkB7ibQqtwII3dS91puxEkzgvg/640?wx_fmt=png" alt=""></p>
<h4 id="四保存模型">四，保存模型</h4>
<p>为减少GPU压力，此处可重启kernel释放显存</p>
<pre><code>import warnings   
warnings.filterwarnings('ignore')  



import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, AutoModel, BitsAndBytesConfig  
from transformers.generation.utils import GenerationConfig  
import torch.nn as nn  
model_name_or_path ='../baichuan-13b'  
ckpt_path = 'baichuan13b_ner'  
tokenizer = AutoTokenizer.from_pretrained(  
    model_name_or_path,  
    trust_remote_code=True  
)  
model_old = AutoModelForCausalLM.from_pretrained(  
    model_name_or_path,  
    trust_remote_code=True,  
    low_cpu_mem_usage=True,  
    torch_dtype=torch.float16,  
    device_map='auto'  
)  
  



from peft import PeftModel  
  
#可能需要5分钟左右  
peft_model = PeftModel.from_pretrained(model_old, ckpt_path)  
model_new = peft_model.merge_and_unload()  
  



from transformers.generation.utils import GenerationConfig  
model_new.generation_config = GenerationConfig.from_pretrained(model_name_or_path)  



from IPython.display import clear_output  
messages = []  
messages.append({&quot;role&quot;: &quot;user&quot;,  
                 &quot;content&quot;: &quot;世界上第二高的山峰是什么？&quot;})  
response = model_new.chat(tokenizer,messages=messages,stream=True)  
for res in response:  
    print(res)  
    clear_output(wait=True)  
</code></pre>
<p>乔戈里峰。世界第二高峰———乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上.</p>
<pre><code>save_path = 'baichuan-13b-ner'  



tokenizer.save_pretrained(save_path)  
model_new.save_pretrained(save_path)  



!cp ../baichuan-13b/*.py  baichuan-13b-ner  
</code></pre>
<h4 id="五使用模型">五，使用模型</h4>
<p>为减少GPU压力，此处可再次重启kernel释放显存。</p>
<pre><code>import torch  
from transformers import AutoTokenizer, AutoModelForCausalLM,AutoConfig, BitsAndBytesConfig  
from transformers.generation.utils import GenerationConfig  
import torch.nn as nn  
  
import warnings  
warnings.filterwarnings('ignore')  
  
model_name_or_path = 'baichuan-13b-ner'  
  
...  
...  
</code></pre>
<p>我们测试一下微调后的效果。</p>
<pre><code>import pandas as pd   
import numpy as np   
import datasets   
from tqdm import tqdm   
  
from sklearn.model_selection import train_test_split  
import pandas as pd   
  
df = pd.read_pickle('dfner_13k.pkl')  
dfdata,dftest = train_test_split(df,test_size=300,random_state=42)  
dftrain,dfval = train_test_split(dfdata,test_size=200,random_state=42)  
...  
...  
...  
  
precision = sum(dftest['tp_cnt'])/sum(dftest['pred_cnt'])  
print('precision = '+str(precision))  
  
recall = sum(dftest['tp_cnt'])/sum(dftest['gt_cnt'])  
print('recall = '+str(recall))  
  
f1 = 2*precision*recall/(precision+recall)  
print('f1_score = '+str(f1))  






precision = 0.9139280125195618  
recall = 0.8427128427128427  
f1_score = 0.876876876876877
</code></pre>
<p>微调后的f1_score为0.8768，相比微调前的f1_score=0.44，取得了不可忽视的巨大提升。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7YIY2pzwPrVHB8pVltLz8VkHHpFTNBVFKKIUzLtmj0dxKVjkYReeQKA/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/4WgILHBwVHibkjD2UfyEny1T6egerAYS7SgdiaMRrUt6ibibo6ia4mcrickfgnFMKg75ILGUdHpFD8FVjIPYpSoH6uRA/640?wx_fmt=png" alt=""></p>
<pre><code>**进技术交流群请添加AINLP小助手微信（id: ainlp2)**   


**请备注具体方向+所用到的相关技术点** 

![](https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJADkmZ2IX6Z23znAibuEevotDMq9iaMxiapK7jfMibiauGFkycicAJEs6x5U9SGyDJZ0S1tRed9TPNUUDQ/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)



**关于AINLP** 

AINLP 是一个有趣有AI的自然语言处理社区，专注于 AI、NLP、机器学习、深度学习、推荐算法等相关技术的分享，主题包括LLM、预训练模型、自动生成、文本摘要、智能问答、聊天机器人、机器翻译、知识图谱、推荐系统、计算广告、招聘信息、求职经验分享等，欢迎关注！加技术交流群请添加AINLP小助手微信(id：ainlp2)，备注工作/研究方向+加群目的。

  


  


![](https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSKABHCqVVQkVYPrM4XY1vsd0iaeuXzyJnoFc8cibd5mYb4wdA3WMQtiaPVmr0XLZHMuVibqWncibpnTSnQ/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)

**阅读至此了，分享、点赞、在看三选一吧🙏** 
</code></pre>
<p>更多AI工具，参考<a href="https://ai123.869hr.uk/">Github-AI123</a>，<a href="https://ai123.869hr.uk/">国内AI123</a></p>



          </div>

<<<<<<< HEAD

=======
 可扫如下微信二维码加好友
>>>>>>> HEAD@{1}

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


