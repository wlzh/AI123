

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>让天下没有难Tuning的大模型：PEFT技术简介 作者： PaperWeekly 来源： [PaperWeekly](https://mp.weixin.qq.com/s/E_0-skD3__w5jLGEJlDpoA) ©作者 | 风飏 **单位 | ** 阿里巴巴 研究方向 | AIOps/NLP 最近，深度学习的研究中出现了许多大型预训练模型，例如 GPT-3、BERT 等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。而其中，ChatGPT 模型因为在对话生成方面的表  | AI123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AI123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://ai123.869hr.uk/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://ai123.869hr.uk/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AI123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">让天下没有难Tuning的大模型：PEFT技术简介</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://ai123.869hr.uk/about>AI123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              June 27, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <pre><code>作者： PaperWeekly  来源： [PaperWeekly](https://mp.weixin.qq.com/s/E_0-skD3__w5jLGEJlDpoA)
</code></pre>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif" alt=""></p>
<p><strong>©作者 |</strong>  风飏</p>
<p>**单位 | ** 阿里巴巴</p>
<p><strong>研究方向 |</strong>  AIOps/NLP</p>
<p>最近，深度学习的研究中出现了许多大型预训练模型，例如 GPT-3、BERT 等，这些模型可以在多种自然语言处理任务中取得优异的性能表现。而其中，ChatGPT 模型因为在对话生成方面的表现而备受瞩目，成为了自然语言处理领域的热门研究方向。</p>
<p>然而，这些大型预训练模型的训练成本非常高昂，需要庞大的计算资源和大量的数据，一般人难以承受。这也导致了一些研究人员难以重复和验证先前的研究成果。为了解决这个问题，研究人员开始研究 Parameter-Efficient Fine-Tuning（PEFT）技术。</p>
<p>PEFT 技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT 技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。下面我们将深入探讨 PEFT 的一些主要做法。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wulOVRfC18yCkd6xXqGq22h6QUk8chptF0fnQ4uXeZtAktYMrWwG2SyQ/640?wx_fmt=png" alt=""></p>
<h4 id="adapter-tuning"><strong>Adapter Tuning</strong></h4>
<p>谷歌的研究人员首次在论文《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT 微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</p>
<p>于是他们设计了如下图所示的 Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。</p>
<p>同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：首先是一个 down-project 层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征；同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为 identity。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1Cxia4a8N9zqkgBaFPHJnb387HyTH5bBXYmo0sefzGz34QWDrOjib4beiaA/640?wx_fmt=png" alt=""></p>
<p>从实验结果来看，该方法能够在只额外对增加的 3.6% 参数规模（相比原来预训练模型的参数量）的情况下取得和 Full-finetuning 接近的效果（GLUE 指标在 0.4% 以内）。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CyuqlhHD9qibDwpjhI8FP0eukHY2zj05e7R1DicElejXU8Ahq4VqlDfLA/640?wx_fmt=png" alt=""></p>
<h4 id="heading">**</h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuhfgUpIfdPSqH8YjjHbCUiaaKsMA36bIMsMtGNKoBcus5py06M0fvx3A/640?wx_fmt=png" alt=""></p>
<h4 id="heading-1">**</h4>
<h4 id="prefix-tuning"><strong>Prefix Tuning</strong></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CIWPj3zgsBlxicQ0AVR0Xd5VoVkNkutfcvseNIWlGGH342dt0niaYNW5A/640?wx_fmt=png" alt=""></p>
<p>Prefix Tuning 方法由斯坦福的研究人员提出，与 Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而 Prefix 则是可以学习的“隐式”的提示。</p>
<p>同时，为了防止直接更新 Prefix 的参数导致训练不稳定的情况，他们在 Prefix 层前面加了 MLP 结构（相当于将 Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果），训练完成后，只保留 Prefix 的参数。</p>
<p>实验结果也说明了 Prefix Tuning 的方式可以取得不错的效果。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CWwM0nBdSZibwfYhLWcRmemOMJ2xpTAsbvEA3y7Ed9QKSBVVvLwD4coA/640?wx_fmt=png" alt=""></p>
<p>除此之外，作者还做了一系列的消融实验说明该方法的有效性：</p>
<ol>
<li>Prefix 长度的影响：不同的任务所需要的 Prefix 的长度有差异。</li>
</ol>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CrrSJN5jUtVwX09cbiabowArR02JL254YdgyNcDvEagq9oAON8n4qs1A/640?wx_fmt=png" alt=""></p>
<ol start="2">
<li>
<p>Full vs Embedding-only：作者对比了 Embedding-only（只有最上层输入处的 Embedding 作为参数更新，后续的参数固定）和 Full（每一层的 Prefix 相关的参数都训练）的方式的效果。</p>
</li>
<li>
<p>Prefixing vs Infixing：对比了 [PREFIX; x; y] 方式与 [x; INFIX; y] 方式的差异，还是 Prefix 方式最好。</p>
</li>
</ol>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1C89BvJxccbD5NahkZfyMRXzaLxmDVFdtgTgKo4LAH2pd7lHOuRKsjibA/640?wx_fmt=png" alt=""></p>
<ol start="4">
<li>Initialization：用任务相关的 Prompt 去初始化 Prefix 能取得更好的效果。</li>
</ol>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CYGc28o8SyHamlgGYAS0ibNqv05nm14gBK4LcYVUf7COC7jiciaaia3GaIA/640?wx_fmt=png" alt=""></p>
<h4 id="heading-2"></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wukOjHSmSsEuRCB0fJu69CtdNgLnvFPDUCgeicOppBKuDvniaD3q8XWQ0Q/640?wx_fmt=png" alt=""></p>
<h4 id="prompt-tuning"><strong>Prompt Tuning</strong></h4>
<p>论文《The Power of Scale for Parameter-Efficient Prompt Tuning》</p>
<p>我给这篇文章取了个新名字：Scale is All You Need，总的来说就是，只要模型规模够大，简单加入 Prompt tokens 进行微调，就能取得很好的效果。</p>
<p>该方法可以看作是 Prefix Tuning 的简化版本，只在输入层加入 prompt tokens，并不需要加入 MLP 进行调整来解决难训练的问题，主要在 T5 预训练模型上做实验。似乎只要预训练模型足够强大，其他的一切都不是问题。作者也做实验说明随着预训练模型参数量的增加，Prompt Tuning 的方法会逼近 Fine-tune 的结果。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1Ch6aa5xic2v53c1TB6AmmUj45MSB0KlC3wCfHc7NPG8oWsb03ulqTUMQ/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CXVGMicO1iclNHXiazF75m0ZjuUyV6Phj0Y6NQBm5x7qYabticCFbjhF9Uw/640?wx_fmt=png" alt=""></p>
<h4 id="heading-3"></h4>
<h4 id="31-实验"><strong>3.1 实验</strong></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1Cq8kLyhLcD6u6We647w5kLNZA06ANoMBd1E6SbB2oPHUA79h7uZHrfw/640?wx_fmt=png" alt=""></p>
<p>作者做了一系列对比实验，都在说明：随着预训练模型参数的增加，一切的问题都不是问题，最简单的设置也能达到极好的效果。</p>
<ul>
<li>
<p>a）Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为 1 也能达到不错的效果，Prompt 长度为 20 就能达到极好效果。</p>
</li>
<li>
<p>b）Prompt 初始化方式影响：Random Uniform 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。</p>
</li>
<li>
<p>c）预训练的方式：LM Adaptation 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。</p>
</li>
<li>
<p>d）微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，zero shot 也能取得不错效果。</p>
</li>
</ul>
<h4 id="heading-4"></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuiaLfO9V4lkD8cXK7ImEicqib5bPGH6syOrWzicR2KaqPyAicMccs8icC03Gw/640?wx_fmt=png" alt=""></p>
<h4 id="heading-5">**</h4>
<h4 id="p-tuning"><strong>P-Tuning</strong></h4>
<h4 id="41-v1"><strong>4.1 V1</strong></h4>
<p>P-Tuning 方法的提出主要是为了解决这样一个问题：大模型的 Prompt 构造方式严重影响下游任务的效果。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1Cg7wo0r5PbzelfEE8GvQCJwotr8QZDw2ehwTH0YEpMNjxJOgjxr9OsQ/640?wx_fmt=png" alt=""></p>
<p>P-Tuning 提出将 Prompt 转换为可以学习的 Embedding 层，只是考虑到直接对 Embedding 参数进行优化会存在这样两个挑战：</p>
<ul>
<li>
<p>Discretenes：对输入正常语料的 Embedding 层已经经过预训练，而如果直接对输入的 prompt embedding 进行随机初始化训练，容易陷入局部最优。</p>
</li>
<li>
<p>Association：没法捕捉到 prompt embedding 之间的相关关系。</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1Cia4ccGo62iaR9V2APgqAA1icTSrEt114b2q4aqno2hTr63x7z2b8rzUvg/640?wx_fmt=png" alt=""></p>
<p>作者在这里提出用 MLP+LSTM 的方式来对 prompt embedding 进行一层处理</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1ChCHntibdWBnS1KkTSG3g4dwnNQJLCDzWPCSiavCXM1tu5OM0xIkrly6g/640?wx_fmt=png" alt=""></p>
<h4 id="411-与-prefix-tuning-的区别"><strong>4.1.1 与 Prefix-Tuning 的区别</strong></h4>
<p>这篇文章（2021-03）和 Prefix-Tuning（2021-01）差不多同时提出，做法其实也有一些相似之处，主要区别在</p>
<ul>
<li>
<p>Prefix Tuning 是将额外的 embedding 加在开头，看起来更像是模仿 Instruction 指令；而 P-Tuning 的位置则不固定。</p>
</li>
<li>
<p>Prefix Tuning 通过在每个 Attention 层都加入 Prefix Embedding 来增加额外的参数，通过 MLP 来初始化；而 P-Tuning 只是在输入的时候加入 Embedding，并通过 LSTM+MLP 来初始化。</p>
</li>
</ul>
<h4 id="heading-6"></h4>
<h4 id="42-v2--"><strong>4.2 V2</strong> ** **</h4>
<p>论文《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》</p>
<p>从标题就可以看出这篇文章的野心，P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果。</p>
<p>那也就是说当前 Prompt Tuning 方法未能在这两个方面都存在局限性。</p>
<ul>
<li>不同模型规模：Prompt Tuning 和 P-tuning 这两种方法都是在预训练模型参数规模够足够大时，才能达到和 Fine-tuning 类似的效果，而参数规模较小时效果则很差。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CuEKYFgrGrgGrtsFtnwRGNPeCsrde8oCBmsWa5ibZFRFAVkUVKCrBLXg/640?wx_fmt=png" alt=""></p>
<ul>
<li>不同任务类型：Prompt Tuning 和 P-tuning 这两种方法在 sequence tagging 任务上表现都很差。</li>
</ul>
<h4 id="heading-7"></h4>
<h4 id="421-主要结构"><strong>4.2.1 主要结构</strong></h4>
<p>相比 Prompt Tuning 和 P-tuning 的方法，P-tuning v2 方法在多层加入了 Prompts tokens 作为输入，带来两个方面的好处：</p>
<ol>
<li>
<p>带来更多可学习的参数（从 P-tuning 和 Prompt Tuning 的 0.1% 增加到0.1%-3%），同时也足够 parameter-efficient。</p>
</li>
<li>
<p>加入到更深层结构中的 Prompt 能给模型预测带来更直接的影响。</p>
</li>
</ol>
<h4 id="heading-8"></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CezrpvnVFIshbUUpwbibEczelcOA785P8RdneqYbFRC2qMPLzVA07Rmg/640?wx_fmt=png" alt=""></p>
<h4 id="422-几个关键设计因素"><strong>4.2.2 几个关键设计因素</strong></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1C2tSPJItBtj6ibnKYFGuwZ2jRS9YEbrOcgHMicfkZcs0KIMEf3ABUiaNOQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p><strong>Reparameterization：</strong> Prefix Tuning 和 P-tuning 中都有 MLP 来构造可训练的 embedding。本文发现在自然语言理解领域，面对不同的任务以及不同的数据集，这种方法可能带来完全相反的结论。</p>
</li>
<li>
<p><strong>Prompt Length：</strong> 不同的任务对应的最合适的 Prompt Length 不一样，比如简单分类任务下 length=20 最好，而复杂的任务需要更长的 Prompt Length。</p>
</li>
<li>
<p><strong>Multi-task Learning</strong>  多任务对于 P-Tuning v2 是可选的，但可以利用它提供更好的初始化来进一步提高性能。</p>
</li>
<li>
<p><strong>Classification Head</strong>  使用 LM head 来预测动词是 Prompt Tuning 的核心，但我们发现在完整的数据设置中没有必要这样做，并且这样做与序列标记不兼容。P-tuning v2 采用和 BERT 一样的方式，在第一个 token 处应用随机初始化的分类头。</p>
</li>
</ul>
<h4 id="heading-9">**</h4>
<h4 id="423-实验结果"><strong>4.2.3 实验结果</strong></h4>
<ul>
<li>不同预训练模型大小下的表现，在小模型下取得与Full-finetuning相近的结果，并远远优于P-Tuning。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1ClqjSyVEtM5AGpDv97MW7lcyoR2iaRs8SVOFWsQzJAlibB54icP0hE4OCg/640?wx_fmt=png" alt=""></p>
<ul>
<li>不同任务下的 P-Tuning v2 效果都很好，而 P-Tuning 和 Prompt Learning 效果不好；同时，采用多任务学习的方式能在多数任务上取得最好的结果。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CLYJzCOntcQz8cP1qK9E06PjLLfbtNmmEwQQ9ib5D6lNveHSEDIOjChg/640?wx_fmt=png" alt=""></p>
<ul>
<li>Verbalizer with LM head v.s. [CLS] label with linear head，两种方式没有太明显的区别</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CIJduann1HdUMgicgrRBBPNjZibyl8szS46yzXSsWLwdibbptZcOibZ1voQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>Prompt depth，在加入相同层数的 Prompts 前提下，往更深层网络加效果优于往更浅层网络（只有 BoolQ 中 17-24 反而低于 1-8 是例外）。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CFtAHrORrpREBhic1zjicEOOnEkTVpoJWXXfa7VNOicUyH9Ef9DZhMhDlA/640?wx_fmt=png" alt=""></p>
<h4 id="heading-10"></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wukGHdevfTibLOpic6945Lrhqmt43pKicyIhGs4m7ANzKOfY9RJgmTicZGdg/640?wx_fmt=png" alt=""></p>
<h4 id="heading-11">**</h4>
<h4 id="lora"><strong>LoRA</strong></h4>
<p>微软和 CMU 的研究者指出，现有的一些 PEFT 的方法还存在这样一些问题：</p>
<ul>
<li>
<p>由于增加了模型的深度从而额外增加了模型推理的延时，如 Adapter 方法</p>
</li>
<li>
<p>Prompt 较难训练，同时减少了模型的可用序列长度，如 Prompt Tuning、Prefix Tuning、P-Tuning 方法</p>
</li>
<li>
<p>往往效率和质量不可兼得，效果差于 full-finetuning</p>
</li>
</ul>
<p>有研究者对语言模型的参数进行研究发现：语言模型虽然参数众多，但是起到关键作用的还是其中低秩的本质维度（low instrisic dimension）。本文受到该观点的启发，提出了 Low-Rank Adaption（LoRA），设计了如下所示的结构，在涉及到矩阵相乘的模块，引入 A、B 这样两个低秩矩阵模块去模拟 Full-finetune 的过程，相当于只对语言模型中起关键作用的低秩本质维度进行更新。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CaXDgBq7yXr3PUPpxjPZFyWcAfISAmKOKeZSFbuyghTaOar8C1Jrcjg/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CoRcZ6xibl4s1d5icLlicEhwMON1fhoZgvaSAypuBnEJ0YF2jX1yW2xzNA/640?wx_fmt=png" alt=""></p>
<p>这么做就能完美解决以上存在的 3 个问题：</p>
<ul>
<li>
<p>相比于原始的 Adapter 方法“额外”增加网络深度，必然会带来推理过程额外的延迟，该方法可以在推理阶段直接用训练好的 A、B 矩阵参数与原预训练模型的参数相加去替换原有预训练模型的参数，这样的话推理过程就相当于和 Full-finetune 一样，没有额外的计算量，从而不会带来性能的损失。</p>
</li>
<li>
<p>由于没有使用 Prompt 方式，自然不会存在 Prompt 方法带来的一系列问题。</p>
</li>
<li>
<p>该方法由于实际上相当于是用 LoRA 去模拟 Full-finetune 的过程，几乎不会带来任何训练效果的损失，后续的实验结果也证明了这一点。</p>
</li>
</ul>
<p>在实验中，研究人员将这一 LoRA 模块与 Transformer 的 attention 模块相结合，在 RoBERTa 、DeBERTa、GPT-2 和 GPT-3 175B 这几个大模型上都做了实验，实验结果也充分证明了该方法的有效性。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CEvXG4bHDAHzOKkwMh5Yct72yUCVTia3QyyBlvS4VsCjoiaUtQSVdl9wQ/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CWCuyXNdn0IOYDcrhGlQ7PmnFCdheXibDY68lE7z8hnQwNdnicBXb8nPA/640?wx_fmt=png" alt=""></p>
<h4 id="51-towards-a-unified-view-of-petl"><strong>5.1 Towards a Unified View of PETL</strong></h4>
<h4 id="heading-12"></h4>
<p>这篇 ICLR2022 的文章研究了典型的 PEFT 方法，试图将 PEFT 统一到一个框架下，找出它们起作用的具体原因，并进行改进。主要研究了三个问题：</p>
<ul>
<li>
<p>典型的PEFT方法有什么联系？</p>
</li>
<li>
<p>典型的PEFT方法中是哪些关键模块在起作用？</p>
</li>
<li>
<p>能否对这些关键模块进行排列组合，找出更有用的 PEFT 方法？</p>
</li>
</ul>
<h4 id="heading-13"></h4>
<h4 id="511-通用形式"><strong>5.1.1 通用形式</strong></h4>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CK1CBKpNb3BFrGONZE7lUY2VNefwmxZlTriciaFNPahR4dkPFaErSKhMA/640?wx_fmt=png" alt=""></p>
<p>通过对 Prefix Tuning 的推导，得出了和 Adapter Tuning 以及 LoRA 形式一致的形式。</p>
<p>更近一步地，可以将这些 Tuning 的方法统一在同一套框架下，</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CKs6tNuxmnkp6CjnboDL1zevCiaa46iaHp7oDDJ6teu1lb0CXc06EY78Q/640?wx_fmt=png" alt=""></p>
<p>包括这几大要素：</p>
<ul>
<li>
<p>的形式</p>
</li>
<li>
<p>嵌入 Transformer 结构的方式（分为 Parrell 和 Sequential 两种。Parallel 指的是在输入层嵌入，这样与原有结构可以并行计算；Sequential 指的是在输出层嵌入，相当于增加了网路的深度，与原有结构存在依赖关系）</p>
</li>
<li>
<p>修改的表示层（主要指对 attention层的修改还是对 ffn 层的修改）</p>
</li>
<li>
<p>组合方式。怎么与原有的参数组合，包括简单相加（Adapter）、门控式（Prefix Tuning）、缩放式（LoRA）三种）</p>
</li>
</ul>
<p>根据这个统一的框架，还另外设计了三种变体Parallel Adapter、Multi-head Parallel Adapter、Scaled Parallel Adapter。</p>
<h4 id="512-一些实验"><strong>5.1.2 一些实验</strong></h4>
<h4 id="哪种嵌入形式更好parallel-or-sequencial"><strong>哪种嵌入形式更好：Parallel or Sequencial？</strong></h4>
<p>答案是：<strong>Parallel 更好</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CtUMicblh2jyMvgsFaTSdcWqfOmeGUdiaqCw29icGWa55XqaFic2HUS972Q/640?wx_fmt=png" alt=""></p>
<p><strong>对哪块结构做修改更好？Attention or FFN？</strong></p>
<ul>
<li><strong>当微调的参数量较多时，从结果来看，对 FFN 层进行修改更好</strong> <strong>。</strong> 一种可能的解释是 FFN 层学到的是任务相关的文本模式，而 Attention 层学到的是成对的位置交叉关系，针对新任务并不需要进行大规模调整。</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1C9aEnquAnVFMYh301REwCH8IOoyYTRc2ASdLBHdHmBKytmA0IAH6hcA/640?wx_fmt=png" alt=""></p>
<ul>
<li><strong>当微调参数量较少（0.1%）时，对 Attention 进行调整效果更好。</strong></li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CfQ9QpTgJQuEZgfbEPVAaia58RCNF6FCEkbbWtyEWU3rSpgIeJTLSPjQ/640?wx_fmt=png" alt=""></p>
<p><strong>哪种组合方式效果更好？</strong></p>
<p><strong>从结果来看，缩放式的组合效果更好。</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1C9kZhkPFiap1QxSgLu3WgYGKjs4YQP2UWicLyLKaWia4o7ibUaBVlDr4TicQ/640?wx_fmt=png" alt=""></p>
<p><strong>5.1.3 结论</strong></p>
<p>基于以上的经验，</p>
<ul>
<li>
<p>Scaled parallel adapter is the best variant to modify FFN</p>
</li>
<li>
<p>FFN can better utilize modification at larger capacities</p>
</li>
<li>
<p>modifying head attentions like prefix tuning can achieve strong performance with only 0.1% parameters</p>
</li>
</ul>
<p>研究者设计出最新的结构 MAM Adapter，取得了最好的效果：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CBXUHMibhEzNPja1lY8nvBicetJZvlBpibEsgyhmibhelNvZzeVKAem7pHQ/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuJXXicvv3JrPNYrFlYadg4ibA8SxC6OvibZyBHGuub04X1AXxeRTC0WUJA/640?wx_fmt=png" alt=""></p>
<p><strong>案例</strong></p>
<p><strong>6.1 典型应用</strong></p>
<p>[1] <a href="https://github.com/mymusise/ChatGLM-Tuning">https://github.com/mymusise/ChatGLM-Tuning</a></p>
<p>一种平价的 Chatgpt 实现方案，基于清华的ChatGLM-6B+ LoRA 进行finetune</p>
<p>[2] <a href="https://github.com/tloen/alpaca-lora">https://github.com/tloen/alpaca-lora</a></p>
<p><strong>6.2 PEFT实现</strong></p>
<p>[1] <a href="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a> huggingface PEFT</p>
<p>[2] <a href="https://github.com/jxhe/unify-parameter-efficient-tuning">https://github.com/jxhe/unify-parameter-efficient-tuning</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRticN5qUOe4cLj1DuRVCU73neXbibnuogRXIYnRDlbcu6l73nRCYEzosIKg1uJTzdp9bia8Ozd8Lmoz58zr93Fiaav/640?wx_fmt=svg" alt=""></p>
<p><strong>参考文献</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRticN5qUOe4cLj1DuRVCU73neXbibnuogRXIYnRDlbcu6l73nRCYEzosIKg1uJTzdp9bia8Ozd8Lmoz58zr93Fiaav/640?wx_fmt=svg" alt=""></p>
<p>[1] Parameter-Efficient Transfer Learning for NLP:</p>
<p><a href="https://arxiv.org/pdf/1902.00751.pdf">https://arxiv.org/pdf/1902.00751.pdf</a></p>
<p>[2] Prefix-Tuning: Optimizing Continuous Prompts for Generation:</p>
<p><a href="https://arxiv.org/pdf/2101.00190.pdf">https://arxiv.org/pdf/2101.00190.pdf</a></p>
<p>[3] The Power of Scale for Parameter-Efficient Prompt Tuning:</p>
<p><a href="https://arxiv.org/pdf/2104.08691.pdf">https://arxiv.org/pdf/2104.08691.pdf</a></p>
<p>[4] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models:</p>
<p><a href="https://arxiv.org/pdf/2106.10199.pdf">https://arxiv.org/pdf/2106.10199.pdf</a></p>
<p>[5] GPT Understands, Too:</p>
<p><a href="https://arxiv.org/pdf/2103.10385.pdf">https://arxiv.org/pdf/2103.10385.pdf</a></p>
<p>[6] TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING:</p>
<p><a href="https://arxiv.org/pdf/2110.04366.pdf">https://arxiv.org/pdf/2110.04366.pdf</a></p>
<p>[7] UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning:</p>
<p><a href="https://arxiv.org/pdf/2110.07577.pdf">https://arxiv.org/pdf/2110.07577.pdf</a></p>
<p>[8] Ladder Side-Tuning：预训练模型的“过墙梯”:</p>
<p><a href="https://kexue.fm/archives/9138">https://kexue.fm/archives/9138</a></p>
<p>[9] INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING:</p>
<p><a href="https://arxiv.org/pdf/2012.13255.pdf">https://arxiv.org/pdf/2012.13255.pdf</a></p>
<p>[10] Prompt-Tuning——深度解读一种新的微调范式:</p>
<p><a href="https://blog.csdn.net/qq_36426650/article/details/120607050">https://blog.csdn.net/qq_36426650/article/details/120607050</a></p>
<p>[11] P-tuning：自动构建模版，释放语言模型潜能:</p>
<p><a href="https://kexue.fm/archives/8295">https://kexue.fm/archives/8295</a></p>
<p>[12] P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks:</p>
<p><a href="https://arxiv.org/pdf/2110.07602.pdf">https://arxiv.org/pdf/2110.07602.pdf</a></p>
<p><strong>更多阅读</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CeVxicPQibujPR9mjkO71icKHY6lJu073TVBTmgiaNCjIewIBKlqEbIwDgw/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CwHhI4VsRC5AyzESjmNzIQA4Hm26zchsibViaqlknyso5pu6fG4T5y6cw/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGHVbjstbXV2ibTlD94OZm1CXVibGNwmpCXhGWvLY1EPhKL1hDLvvU4wxR1HWUzgN0q6pibQyxkvUo8A/640?wx_fmt=png" alt=""></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHHMXQ2IicFvJwssWxgWhKuK7ulQVyw7gPTxZia00vCxia2vzhRH6pGq8t1FN1zY48ibULAEZpic41k6eg/640?wx_fmt=gif" alt=""></p>
<p><strong>#投 稿 通 道#</strong></p>
<p>** 让你的文字被更多人看到 **</p>
<p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></p>
<p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p>
<p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong> ，也可以是<strong>学术热点剖析</strong> 、<strong>科研心得</strong> 或<strong>竞赛经验讲解</strong> 等。我们的目的只有一个，让知识真正流动起来。</p>
<p>📝 <strong>稿件基本要求：</strong></p>
<p>• 文章确系个人<strong>原创作品</strong> ，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p>
<p>• 稿件建议以 <strong>markdown</strong>  格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p>
<p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong> ，具体依据文章阅读量和文章质量阶梯制结算</p>
<p>📬 <strong>投稿通道：</strong></p>
<p>• 投稿邮箱：hr@paperweekly.site</p>
<p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p>
<p>• 您也可以直接添加小编微信（<strong>pwbot02</strong> ）快速投稿，备注：姓名-投稿</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmic1CRCSOKfDibC3dZ4BaJuYyYTWJyw8gFxqon34STk3icf9aJbY4rqMpmhNjTGJXIGGFsCdTBHy3Tw/640?wx_fmt=png" alt=""></p>
<p><strong>△长按添加PaperWeekly小编</strong></p>
<p>🔍</p>
<p>现在，在**「知乎」** 也能找到我们了</p>
<p>进入知乎首页搜索**「PaperWeekly」**</p>
<p>点击**「关注」** 订阅我们的专栏吧</p>
<p>·</p>
<p>·</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnZ3nlEAOI3MyTd7jqeD6cq8uTbkM2xZNpribyNr9liaPJ722zaHxd0YpQvib2nxOYmWibydCVY7W94ew/640?wx_fmt=jpeg" alt=""></p>



          </div>

<<<<<<< HEAD

=======
 可扫如下微信二维码加好友
>>>>>>> HEAD@{1}

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


