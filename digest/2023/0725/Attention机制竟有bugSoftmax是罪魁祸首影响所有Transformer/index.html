

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>Attention机制竟有bug，Softmax是罪魁祸首，影响所有Transformer 作者： 机器之心 来源： 机器之心 机器之心报道 机器之心编辑部 「大模型开发者，你们错了。」 「我发现注意力公式里有个 bug，八年了都没有人发现。所有 Transformer 模型包括 GPT、LLaMA 都受到了影响。」 昨天，一位名叫 Evan Miller 的统计工程师的话在 AI 领域掀起了轩然大波。  | AI123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AI123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://ai123.869hr.uk/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://ai123.869hr.uk/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AI123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">Attention机制竟有bug，Softmax是罪魁祸首，影响所有Transformer</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://ai123.869hr.uk/about>AI123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              July 25, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYb2hlj8VmwNzzTXnCgc7LqicJJgV4cibozia7vTVybJGKlrQB0GpyYRJew/640?wx_fmt=png" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： 机器之心  来源： <a href="https://mp.weixin.qq.com/s/cSwWapqFhxu9zafzPUeVEw">机器之心</a></p>
<p>机器之心报道</p>
<p><strong>机器之心编辑部</strong></p>
<blockquote>
<p>「大模型开发者，你们错了。」</p>
</blockquote>
<p>「我发现注意力公式里有个 bug，八年了都没有人发现。所有 Transformer 模型包括 GPT、LLaMA 都受到了影响。」</p>
<p>昨天，一位名叫 Evan Miller 的统计工程师的话在 AI 领域掀起了轩然大波。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKY3TxZsoTbdpRA5OymJISP18RScgPIIJUsTUCnEJYcq5UpwVtoUWvj0w/640?wx_fmt=png" alt=""></p>
<p>我们知道，机器学习中注意力公式是这样的：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYDKMMfeRGXIlJmAibTu9kGiae8auMhOR9qQQ8aiaqx2OLOAGugkoILI7vg/640?wx_fmt=png" alt=""></p>
<p>自 2017 年 Transformer 问世，这个公式已被广泛使用，但现在，Evan Miller 发现这个公式是错的，有 bug！</p>
<p>Evan Miller 的这篇博客解释了当前流行的 AI 模型如何在关键位置出现错误，并使得所有 Transformer 模型都难以压缩和部署。</p>
<p>总结而言，Evan Miller 引入了一种新函数 Quiet Attention，也叫 Softmax_1，这是对传统 softmax 函数的创新调整。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYz4XAWVNmjgFOajM701snrPZCxpRo9fFVna0BrjMWLqYaFlvBneEDdQ/640?wx_fmt=png" alt=""></p>
<p>有网友对该博客总结出了一个「太长不看版」。博客作者建议在注意力机制使用的 softmax 公式分母上加 1（不是最终输出 softmax）。注意力单元中的 softmax 使其可以将键 / 查询匹配作为概率；这些概率支持一个键 - 值查找的连续值版本（我们得到的权重不是一个查找的 1/0 输出，而是高权重 = 所需的键 - 值查找）。</p>
<p>分母上加 1 将改变注意力单元，不再使用真实的权重概率向量，而是使用加起来小于 1 的权重。其动机是该网络可以学习提供高权重，这样调整后的 softmax 非常接近概率向量。同时有一个新的选项来提供 all-low 权重（它们提供 all-low 输出权重），这意味着它可以选择不对任何事情具有高置信度。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKY0xVb4vrGGYTVKQsDUQXqRp1Al8gzb4SQs2pibNlhtHu6AVtSIekI2Gw/640?wx_fmt=png" alt=""></p>
<p>有人甚至猜测「这就是微软 RetNet 比 transformer 性能更优的原因？」</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYIgibqA51k8wUdOeBgbPJk6ZicGAJ8wb8h7q3o8H5UMa9JZichAibuPS8JQ/640?wx_fmt=png" alt=""></p>
<p>还有网友表示，这项研究可以促进 LLM 的改进，从而极大对权重进行压缩，使得较小的模型媲美较大的模型：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYAl7ibVDQzH4HP8wM5ZfpmHoqkxiaMicuPg3jD7yz2rVmZ5c7ZCAsdxzVg/640?wx_fmt=png" alt=""></p>
<p>Miller 表示：你可以像使用传统的 softmax 函数一样使用 Softmax_1 函数，示例如下。</p>
<hr>
<pre><code>import torch
from softmax_one.softmax_one import softmax_one
x = torch.randn(5)y = softmax_one(x, dim=0)
</code></pre>
<p>基于这样的修改，Miller 还做了实验，结果如下：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYSXBISSaTEiaJMsAuuYjyQKGQ53uKycNl6uV4iapw7V7ib8d25GgFNQGdw/640?wx_fmt=png" alt=""></p>
<p>接下来我们看看 Miller 到底发现了什么错误。</p>
<p><strong>异常值</strong></p>
<p>Evan Miller 是在阅读关于量化的论文时发现了这个 bug。当前，内存和存储已经成为限制人工智能发展的重要因素。人们一直在努力压缩模型，并尝试在云端、在边缘设备上运行大型语言模型（LLM）。</p>
<p>在计算机中，信息是用二进制数据流来存储的。如果数据流是高度可预测的，例如总是包含在有限的范围内，那么我们就可以用相对较少的位（bit）来存储它们。反之，如果一串数字是不可预测的，可能是千载难逢的巨大数字，我们就需要更多的二进制数字来编码和存储。而 Transformer 模型包含一些异常值权重。</p>
<p>在高通 AI Research 6 月发表的一篇论文《Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing》中，研究团队将这些异常值的存在追溯到注意力机制的 softmax 函数。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYyIxZOEZddOYTtVpkprHb2IgMcDma0iaIK2z3gdsTPTzz8MwaOe9sIjA/640?wx_fmt=png" alt=""><em>高通论文地址：https://arxiv.org/abs/2306.12929</em></p>
<p>这听起来令人意外，但 Evan Miller 认为这是对的，并进一步发现 softmax 函数存在一个错误。</p>
<p>我们来看下 Evan Miller 是如何说明 softmax 函数在注意力机制方面并不是一个合适的工具的。</p>
<p><strong>Softmax 引出的问题</strong></p>
<p>为什么说 softmax 不适合注意力机制，这还得从注意力机制可以做什么开始。</p>
<p>一般来讲，数值错误一般是由程序错误引起的，然而，当程序没有错误，这时就需要从修复复杂的数学公式入手，耗费大量时间。</p>
<p>Evan Miller 大概阅读了 50 篇 arXiV 论文才有点眉目。Miller 从「输入嵌入」入手，我们可以将「输入嵌入」理解为一个浮点向量，代表输入字符串中的一个单词。</p>
<p>举例来说，Meta 最近推出的 LLaMA 2 模型使用了一个长度为 3204 的嵌入向量，以半精度浮点数表示，这仅仅是为了表示词汇表中的一个单词，而词汇表通常包含 30000 到 50000 个条目（entry）。意味着一个单词的嵌入向量占用 6KB + 的存储空间。随着技术的发展，「输入嵌入」的长度逐渐增加，所占存储空间也随之增加。</p>
<p>如果你是一个对存储占用非常敏感的 C 程序员，你可能接受不了这一数字，明明是 2 字节就能存储的东西，为什么偏偏要用 6KB？如果按照 2 字节来计算，假如词汇量少于 2^16=65384，那么我们只需要 16 位来表示一个条目。</p>
<p>但是，实际上 Transformer 的工作原理是这样的：它将输入向量转换为大小相同的输出向量，最终的 6KB 输出向量用来预测下一个 token。运行中，Transformer 每一层的工作流都将信息添加到原始的单词向量中。在这其中，还用到了残差连接：所有的注意力机制都在为原始的两个字节的信息添加补充材料，从而是的 LLM 能够分析更长的上下文。</p>
<p>Transformer 的最后一步是将这个输出向量与一个矩形矩阵相乘，并将得到的词汇长度向量压缩到一个 softmax 函数中，将这些指数化的输出视为下一个 token 的概率。这是合理的，但众所周知，这并不完全正确，因为我们不能确定这些输出概率是正确的。相反，每个 Transformer 实现和其衍生版本都使用采样机制来隐藏 softmax 过度表示概率较低的事实。</p>
<p>接下里，Miller 介绍了 softmax 的发展史。softmax 最初出现在统计学中，最早作为一种基于能级预测状态分布的方法，其形式如下：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYr8wqNZt5A7mmXvuIERGqxuiaOABooeicsTBYCdMvJnRoAaYqWUiaHzlxg/640?wx_fmt=png" alt=""></p>
<p>之后经济学家又将其修改为</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYQ79hEm5KPNXgiaxY9DRrDgSZS6EljKyrBK7ZjZHiaiaKY3fXgsduPXZ2g/640?wx_fmt=png" alt=""></p>
<p>这一修改，softmax 才拥有了多项逻辑函数。由于 Miller 对 softmax 函数的研究颇深，因而，他能识别出 softmax 使用不恰当的地方。</p>
<p>Softmax 应用广泛，在物理学中，它非常有效；在经济学中，它可能不那么准确；但将其应用到机器学习领域时，只要涉及离散选择，它似乎总是有效的：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYb2hlj8VmwNzzTXnCgc7LqicJJgV4cibozia7vTVybJGKlrQB0GpyYRJew/640?wx_fmt=png" alt=""></p>
<p>Miller 进一步表示，softmax 的关键在于，如果你不想保留一些项，必须对 softmax 进行修改，否则结果就会产生扭曲。</p>
<p>举例来说，在 LLM 上下文中，扭曲产生的原因是对非语义 token（逗号等）进行大量加权导致的，这些较高的权重成为难以压缩的异常值，使得研究变得更加困难。来自高通的 AI 研究员也发现了这一现象，在 LLM 中，97% 以上的异常激活发生在空格和标点符号位置上。</p>
<p>接下来，Miller 介绍了 softmax 是如何在注意力中使用的，从而发现问题到底出现在哪里：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYia84ViaF75e0GuHlkGes86eIovWGVHIw5VWy1dL0XWZowBWa81Ulpbjg/640?wx_fmt=png" alt=""></p>
<p>对上述公式进行分解，在仅解码器模型中，𝑄、𝐾和𝑉源自相同的输入序列。它们又不完全相同，即投影方式不同。但在每一层中，它们都以相同的注释嵌入向量开始。</p>
<p>𝑄𝐾^𝑇项用于寻找不同位置 token 向量之间的相关性，实质上构建了一个相关性矩阵（点积按<img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYia30Epb8xiaCofzqWClFSZgng5283gqsc4TXJwFiaIyp3y9J7P7OqRw0A/640?wx_fmt=png" alt="">缩放），其中每一列和每一行对应一个 token 位置。然后，对这个方阵的每一行进行 softmax 操作，得到的概率用作𝑉矩阵中值向量的混合函数。概率混合后的𝑉与输入向量相加，将求和结果传递给神经网络进行进一步处理。</p>
<p>多头注意力每层并行执行多次上述过程。从本质上讲，这种方法划分了嵌入向量，每个头使用整个向量中的信息来注释输出向量的一个（非重叠）片段。这就是原始 Transformer 论文中的串联操作。</p>
<p>使用 softmax 的问题在于，它强制每个注意力头进行注释，即使没有信息可添加到输出向量中。</p>
<p><strong>Softmax_1 和 QuietAttention</strong></p>
<p>来了，在这里你将看到 Softmax Super-Mod 点燃了 LLM 黑客频道。</p>
<p>有点失望，对吧？Miller 所做的只是在分母上加 1。如果想要的话，这可以让该向量作为一个趋于 0 的整体。否则只会将值缩小一点，并且缩小的值会在归一化过程中得到补偿，这在注意力之后发生。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYiaToia7cicBbAib3aIyoEwSwVqr4zGBibuy2dyibMvYR3OpJia4vOcvUsYuSw/640?wx_fmt=png" alt=""></p>
<p>当 𝑥 中的条目显著小于零并且模型试图完全避免注释时，主要的区别在于负值限制。将如下原始 softmax 的限制行为</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKY45WURSOvOOCnMAnm9ehehuytm37DdQIH2gSb0ic7f9Zib2TFjjBNtibzQ/640?wx_fmt=png" alt=""></p>
<p>与新的、改进后的 softmax_1 相比较。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKY4TC9icwaBAbuRnibpI8vicnickKFEd2VbiapGytWxib9qpucKYGP2sb7Tfpw/640?wx_fmt=png" alt=""></p>
<p>Vanilla softmax 将始终释出相同的总权重；softmax_1 看起来大部分相同，但在负象限中有一个「逃出口」（escape hatch）。需要明确的是，这里的核心问题在本质上是数学而非数值问题。额外的精度并不能拯救 softmax，所有的 Transformers 都会受到影响。</p>
<p>你还可以观察到关于 softmax_1 的其他一些事项。导数是正的，所以总是有一个非零梯度，并且它的和介于 0 和 1 之间，所以输出不会失控。该函数保持以下属性</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYIy9rOEUhaYR1AhG1dKIzHFDVYJzTgiaGe89UKPS5spibCVFlmXmk0Ogw/640?wx_fmt=png" alt=""></p>
<p>即输出向量中的相对值不变。</p>
<p>最开始 Miller 打算将这个函数称为 ghostmax，这是因为你可以认为<img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKYqWjXaJPcoHiaqX0lBnGQclRibcmYMRiajvdcz873fdx8XOGEAh4NEZ1DA/640?wx_fmt=png" alt="">中有一个额外的零值条目，并且 V 矩阵中有一个能够衰减结果的零向量。</p>
<p>尽管 softmax_1 表面上看起来很无聊，但 Miller 99.44% 确信它将解决异常值反馈循环，使量化成为级联研究的主题。Miller 表示，如果你想进行一些实验来证明他是对的，可以联系他。他将撰写一篇论文。</p>
<p>改进后的机制可以被称为 QuietAttention，它允许注意力头保持「沉默」。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8j71pMzUWHxW4ibuqFh6vKY0hcJJKvCeH9Qhx4jAAfVjAAIS0olEdWicb326qWrzv7aibrpBTrPrQ7A/640?wx_fmt=png" alt=""></p>
<p>Miller 认为很快可以整合一项测试：如果你在每个输入上下文的前面加上一个零向量，并确保你选择的神经网络不添加任何偏差（包括位置编码），那么零在通过时不会改变，并对每个后续的 softmax 分母添加 unity 产生影响。这样你不会因为处理梯度代码失去理智。Miller 认为这可以通过使用固定嵌入和特殊前缀 token 的 LLaMA 模型来完成。</p>
<p>你仍然需要重新训练模型，因此暂时不要在树莓派（RPi）上尝试此操作。但 Miller 想知道这些权重峰度和激活无穷范数在运行几次后是什么样子的。他认为这会成为有影响力的研究，无论是高通 AI Research 团队的论文，还是 LLM 黑客频道有人计算出 biblatex，但自己最先发现的。</p>
<ul>
<li>
<p>项目地址：https://github.com/kyegomez/AttentionIsOFFByOne</p>
</li>
<li>
<p>博客链接：https://www.evanmiller.org/attention-is-off-by-one.html?continueFlag=5d0e431f4edf1d8cccea47871e82fbc4</p>
</li>
</ul>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqs3dicZ1ibGicA1BHoaoMf3Im3KMaWO4e4OeUMNhIEuYfTib5b6fMialKic9qVWic4OvTyOwxSO6cLhyiaQ/640?wx_fmt=png" alt=""></p>
<p>© THE END</p>
<p>转载请联系本公众号获得授权</p>
<p>投稿或寻求报道：content@jiqizhixin.com</p>
<p>更多AI工具，参考<a href="https://ai123.869hr.uk/">Github-AI123</a>，<a href="https://ai123.869hr.uk/">国内AI123</a></p>



          </div>

<<<<<<< HEAD

=======
 可扫如下微信二维码加好友
>>>>>>> HEAD@{1}

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


