

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>大模型实践总结-最新版（20230703） 作者： 吃果冻不吐果冻皮 来源： 吃果冻不吐果冻皮 ####**【点击】加入大模型技术交流群** 随着ChatGPT的迅速出圈，加速了大模型时代的变革。对于以Transformer、MOE结构为代表的大模型来说，传统的单机单卡训练模式肯定不能满足上千  | AI123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AI123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://ai123.869hr.uk/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://ai123.869hr.uk/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AI123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">大模型实践总结-最新版（20230703）</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://ai123.869hr.uk/about>AI123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              August 18, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJPvLIZaMPAG0ia7K45nTXXqVLTAWictjTcvf0UU4JOd0EicyccviaovV3iahA/640?wx_fmt=png" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： 吃果冻不吐果冻皮  来源： <a href="https://mp.weixin.qq.com/s/6n8KiOBbro6csGET_mMaYw">吃果冻不吐果冻皮</a></p>
<p>####**<a href="http://mp.weixin.qq.com/s?__biz=MzU3Mzg5ODgxMg==&amp;mid=2247485022&amp;idx=1&amp;sn=989a9f837a9950d1c84cd7bb53fa78a2&amp;chksm=fd3bef0dca4c661b4b7bc87410f488b26da6ec86d62825de51cdc590f940f26c06c9b70e5b60&amp;scene=21#wechat_redirect">【点击】加入大模型技术交流群** </a></p>
<p>随着ChatGPT的迅速出圈，加速了大模型时代的变革。对于以Transformer、MOE结构为代表的大模型来说，传统的单机单卡训练模式肯定不能满足上千（万）亿级参数的模型训练，这时候我们就需要解决内存墙和通信墙等一系列问题，在单机多卡或者多机多卡进行模型训练。</p>
<p>最近，我也在探索大模型相关的一些技术，下面做一个简单的总结，后续争取每一个季度更新一次，目前最新更新为2023.07.03，本文主要涉及<strong>AI集群、AI集群通信、大模型训练（参数高效微调）、大模型推理加速、大模型评估、大模型生态相关技术等</strong> 相关内容，同时，也对之前写过的一些大模型相关的文章进行了汇总，篇幅太长，建议先收藏后再阅读。</p>
<h4 id="ai-集群">AI 集群</h4>
<p>由于目前只有3台A800 GPU服务器（共24卡），基于目前现有的一些AI框架和大模型，无法充分利用3台服务器。比如：OPT-66B一共有64层Transformer，当使用Alpa进行流水线并行时，通过流水线并行对模型进行切分，要么使用16卡，要么使用8卡，没法直接使用24卡，因此，GPU服务器最好是购买偶数台（如：2台、4台、8台）。</p>
<p>具体的硬件配置如下：</p>
<p>*<strong>CPUs:</strong>  每个节点具有 1TB 内存的 Intel CPU，物理CPU个数为64，每颗CPU核数为16。</p>
<p>*<strong>GPUs:</strong>  24 卡 A800 80GB GPUs ，每个节点 8 个 GPU（3 个节点）。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJPtaJfs4hQ2Rg01Y1S4AqG23XpXiax18IwuJEedSNuebustibZlSufsnSA/640?wx_fmt=png" alt="">image.png</p>
<p>目前使用Huggingface Transformers和DeepSpeed进行通过数据并行进行训练（pretrain），单卡可以跑三百亿参数（启用ZeRO-2或ZeRO-3），如OPT-30B，具体训练教程参考官方样例。</p>
<p>使用Alpa进行流水线并行和数据并行进行训练（fine tuning）时，使用了3台共24卡（PP：12，DP：2）进行训练OPT-30B，具体训练教程参考官方样例。但是进行模型训练之前需要先进行模型格式转换，将HF格式转换为Alpa格式的模型文件，具体请参考官方代码。如果不想转换，官网也提供了转换好的模型格式，具体请参考文档：Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJPUA2hDnWbibx1jNYx0twiaIia4mBQicpiaQD4uWTcUSom6x38U5Hd8Du647w/640?wx_fmt=png" alt="">image.png</p>
<p><strong>20230703</strong> ：前几天对H800进行过性能测试，整体上来说，对于模型训练（Huggingface Transformers）和模型推理（FasterTransformer）都有30%左右的速度提升。但是对于H800支持的新数据类型FP8，目前很多开源框架暂不支持，虽然，Nvidia自家一些开源框架支持该数据类型，目前不算太稳定。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJP36JamJYVvuEhFGib1ApiaNTTicZHDl1xXo6Lia8GV6wiamW0xfQ8YfRNRtg/640?wx_fmt=png" alt="">image.png</p>
<h4 id="ai处理器加速卡-新增20230630">AI处理器（加速卡）-新增（2023.06.30）</h4>
<p>目前，主流的AI处理器无疑是NVIDIA的GPU，NVIDIA的GPU产品主要有GeForce、Tesla和Quadro三大系列，虽然，从硬件角度来看，它们都采用同样的架构设计，也都支持用作通用计算(GPGPU)，但因为它们分别面向的目标市场以及产品定位的不同，这三个系列的GPU在软硬件的设计和支持上都存在许多差异。其中，GeForce为消费级显卡，而Tesla和Quadro归类为专业级显卡。GeForce主要应用于游戏娱乐领域，而Quadro主要用于专业可视化设计和创作，Tesla更偏重于深度学习、人工智能和高性能计算。</p>
<ul>
<li>
<p>Tesla：A100（A800）、H100（H800）、A30、A40、V100、P100&hellip;</p>
</li>
<li>
<p>GeForce：RTX 3090、RTX 4090 &hellip;</p>
</li>
<li>
<p>Quadro：RTX 6000、RTX 8000 &hellip;</p>
</li>
</ul>
<p>其中，A800/H800是针对中国特供版（低配版），相对于A100/H100，主要区别：</p>
<ul>
<li>
<p>A100的Nvlink最大总网络带宽为600GB/s，而A800的Nvlink最大总网络带宽为400GB/s。</p>
</li>
<li>
<p>H100的Nvlink最大总网络带宽为900GB/s，而A800的Nvlink最大总网络带宽为400GB/s。</p>
</li>
</ul>
<p><strong>其他的一些国外AI处理器</strong> （加速卡）：</p>
<ul>
<li>
<p>AMD：GPU MI300X</p>
</li>
<li>
<p>Intel：Xeon Phi</p>
</li>
<li>
<p>Google：TPU</p>
</li>
</ul>
<p><strong>国产AI处理器</strong> （加速卡）：</p>
<ul>
<li>
<p>华为：昇腾910（用于训练和推理），昇腾310（用于推理）。采用自家设计的达芬奇架构。</p>
</li>
<li>
<p>海光DCU：8100系列（深算一号），以GPGPU架构为基础。</p>
</li>
<li>
<p>寒武纪：思元370、思元590。</p>
</li>
<li>
<p>百度：昆仑芯，采用的是其自研XPU架构。</p>
</li>
<li>
<p>阿里：含光800。</p>
</li>
</ul>
<h4 id="大模型算法">大模型算法</h4>
<p><strong>模型结构</strong> ：</p>
<p>目前主流的大模型都是Transformer、MOE结构为基础进行构建，如果说Transformer结构使得模型突破到上亿参数量，MoE 稀疏混合专家结构使模型参数量产生进一步突破，达到数万亿规模。</p>
<p><strong>大模型算法</strong> ：</p>
<p>下图详细展示了AI大模型的发展历程：<img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJPvLIZaMPAG0ia7K45nTXXqVLTAWictjTcvf0UU4JOd0EicyccviaovV3iahA/640?wx_fmt=png" alt=""></p>
<p>可以说，Transformer 开创了继 MLP 、CNN和 RNN之后的第四大类模型。而基于Transformer结构的模型又可以分为Encoder-only、Decoder-only、Encoder-Decoder这三类。</p>
<ul>
<li>
<p>仅编码器架构（Encoder-only）：自编码模型（破坏一个句子，然后让模型去预测或填补），更擅长理解类的任务，例如：文本分类、实体识别、关键信息抽取等。典型代表有：Bert、RoBERTa等。</p>
</li>
<li>
<p>仅解码器架构（Decoder-only）：自回归模型（将解码器自己当前步的输出加入下一步的输入，解码器融合所有已经输入的向量来输出下一个向量，所以越往后的输出考虑了更多输入），更擅长生成类的任务，例如：文本生成。典型代表有：GPT系列、LLaMA、OPT、Bloom等。</p>
</li>
<li>
<p>编码器-解码器架构（Encoder-Decoder）：序列到序列模型（编码器的输出作为解码器的输入），主要用于基于条件的生成任务，例如：翻译，概要等。典型代表有：T5、BART、GLM等。</p>
</li>
</ul>
<h4 id="大语言模型">大语言模型</h4>
<p><strong>目前业界可以下载到的一些大语言模型</strong> ：</p>
<ul>
<li>
<p>ChatGLM-6B / ChatGLM2-6B ：清华开源的中英双语的对话语言模型。目前，第二代ChatGLM在官网允许的情况下可以进行商用。</p>
</li>
<li>
<p>GLM-10B/130B ：双语（中文和英文）双向稠密模型。</p>
</li>
<li>
<p>OPT-2.7B/13B/30B/66B ：Meta开源的预训练语言模型。</p>
</li>
<li>
<p>LLaMA-7B/13B/30B/65B ：Meta开源的基础大语言模型。</p>
</li>
<li>
<p>Alpaca（LLaMA-7B）：斯坦福提出的一个强大的可复现的指令跟随模型，种子任务都是英语，收集的数据也都是英文，因此训练出来的模型未对中文优化。</p>
</li>
<li>
<p>BELLE（BLOOMZ-7B/LLaMA-7B/LLaMA-13B）：本项目基于 Stanford Alpaca，针对中文做了优化，模型调优仅使用由ChatGPT生产的数据（不包含任何其他数据）。</p>
</li>
<li>
<p>Bloom-7B/13B/176B：可以处理46 种语言，包括法语、汉语、越南语、印度尼西亚语、加泰罗尼亚语、13 种印度语言（如印地语）和 20 种非洲语言。其中，Bloomz系列模型是基于 xP3 数据集微调。 推荐用于英语的提示（prompting）；Bloomz-mt系列模型是基于 xP3mt 数据集微调。推荐用于非英语的提示（prompting）。</p>
</li>
<li>
<p>Vicuna(7B/13B)：由UC Berkeley、CMU、Stanford和 UC San Diego的研究人员创建的 Vicuna-13B，通过在 ShareGPT 收集的用户共享对话数据中微调 LLaMA 获得。其中，使用 GPT-4 进行评估，发现 Vicuna-13B 的性能在超过90%的情况下实现了与ChatGPT和Bard相匹敌的能力；同时，在 90% 情况下都优于 LLaMA 和 Alpaca 等其他模型。而训练 Vicuna-13B 的费用约为 300 美元。不仅如此，它还提供了一个用于训练、服务和评估基于大语言模型的聊天机器人的开放平台：FastChat。</p>
</li>
<li>
<p>Baize：白泽是在LLaMA上训练的。目前包括四种英语模型：白泽-7B、13B 、 30B（通用对话模型）以及一个垂直领域的白泽-医疗模型，供研究 / 非商业用途使用，并计划在未来发布中文的白泽模型。白泽的数据处理、训练模型、Demo 等全部代码已经开源。</p>
</li>
<li>
<p>LLMZoo：来自香港中文大学和深圳市大数据研究院团队推出的一系列大模型，如：Phoenix（凤凰） 和 Chimera等。</p>
</li>
<li>
<p>MOSS：由复旦 NLP 团队推出的 MOSS 大语言模型。</p>
</li>
<li>
<p>baichuan-7B：由百川智能推出的大模型，可进行商用。</p>
</li>
<li>
<p>CPM-Bee：百亿参数的开源中英文双语基座大模型，可进行商用。</p>
</li>
</ul>
<p>&hellip;</p>
<blockquote>
<p>20230325（当时官方还未开源训练代码，目前直接基于官方的训练代码即可）:</p>
<p>前两天测试了BELLE，对中文的效果感觉还不错。具体的模型训练（预训练）方法可参考Hugingface Transformers的样例，SFT（指令精调）方法可参考Alpaca的训练代码。</p>
</blockquote>
<p>从上面可以看到，开源的大语言模型主要有三大类：GLM衍生的大模型（wenda、ChatSQL等）、LLaMA衍生的大模型（Alpaca、Vicuna、BELLE、Phoenix、Chimera等）、Bloom衍生的大模型（Bloomz、BELLE、Phoenix等）。</p>
<p>模型训练数据量模型参数训练数据范围词表大小</p>
<p>LLaMA
1T～1.4T tokens(其中，7B/13B使用1T，33B/65B使用1.4T)
7B～65B
以英语为主要语言的拉丁语系
32000</p>
<p>ChatGLM-6B
约 1T tokens
6B
中文、英语
130528</p>
<p>Bloom
1.6TB预处理文本，转换为 350B 唯一 tokens
300M~176B
46种自然语言，13种编程语言
250680</p>
<p>从表格中可以看到，对于像ChatGLM-6B、LLaMA、Bloom这类大模型，要保证基座模型有比较好的效果，至少需要保证上千亿、万亿级的Token量。</p>
<p>目前来看，LLaMA无疑是其中最闪亮的星。但是国内关于LLaMA比较大的一个争论就是LLaMA是以英语为主要语言的拉丁语系上进行训练的，LLaMA词表中的中文token比较少（只有几百个），需不需要扩充词表？如果不扩充词表，中文效果会不会比较差？</p>
<ul>
<li>
<p>如果不扩充词表，对于中文效果怎么样？根据Vicuna官方的报告，经过Instruction Turing的Vicuna-13B已经有非常好的中文能力。</p>
</li>
<li>
<p>LLaMA需不需要扩充词表？根据Chinese-LLaMA-Alpaca和BELLE的报告，扩充中文词表，可以提升中文编解码效率以及模型的性能。但是扩词表，相当于从头初始化开始训练这些参数。如果想达到比较好的性能，需要比较大的算力和数据量。同时，Chinese-LLaMA-Alpaca也指出在进行第一阶段预训练（冻结transformer参数，仅训练embedding，在尽量不干扰原模型的情况下适配新增的中文词向量）时，模型收敛速度较慢。如果不是有特别充裕的时间和计算资源，建议跳过该阶段。因此，虽然扩词表看起来很诱人，但是实际操作起来，还是很有难度的。</p>
</li>
</ul>
<p>下面是BELLE针对是否扩充词表，数据质量、数据语言分布、数据规模对于模型性能的对比：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJPlo8O91zibKVj4C8XrMdpkjl19Ecr8IY4pwbs8wic3rXzhwtD9BhgVgGQ/640?wx_fmt=png" alt="">image.png</p>
<p>其中，<strong>BELLE-0.5M-CLEAN</strong> 是从230万指令数据中清洗得到0.5M数据（包含单轮和多轮对话数据）。LLaMA-7B-EXT是针对LLaMA做了中文词表扩充的预训练模型。</p>
<p>下面是Chinese-LLaMA-Alpaca针对中文Alpaca-13B、中文Alpaca-Plus-7B、中文Alpaca-Plus-13B的效果对比：</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJP9faTCX4ZJib5082MF67ypAJcAvF3m5bunEqLdY0T5q7vVnKHv8LHAjA/640?wx_fmt=png" alt="">image.png</p>
<p>其中，Plus系列Alpaca是在原版LLaMA的基础上扩充了中文词表，使用了120G中文通用纯文本数据进行二次预训练。</p>
<p>因此，如果既想要中文词表，又没有很大的算力，那建议直接使用ChatGLM-6B或者使用BELLE和Chinese-LLaMA-Alpaca进行中文词表扩充后训练好的模型作为Base模型。</p>
<h4 id="多模态大模型">多模态大模型</h4>
<p><strong>目前业界可以下载到的一些多模态大模型</strong> ：</p>
<ul>
<li>
<p>MiniGPT-4：沙特阿拉伯阿卜杜拉国王科技大学的研究团队开源。</p>
</li>
<li>
<p>LLaVA：由威斯康星大学麦迪逊分校，微软研究院和哥伦比亚大学共同出品。</p>
</li>
<li>
<p>VisualGLM-6B：开源的，支持<strong>图像、中文和英文</strong> 的多模态对话语言模型，语言模型基于 ChatGLM-6B，具有 62 亿参数；图像部分通过训练 BLIP2-Qformer 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。</p>
</li>
<li>
<p>VisCPM：于CPM基础模型的中英双语多模态大模型。</p>
</li>
</ul>
<h4 id="领域大模型-新增20230630">领域大模型-新增（2023.06.30）</h4>
<h4 id="金融领域大模型">金融领域大模型</h4>
<ul>
<li>
<p>轩辕：在BLOOM-176B的基础上针对中文通用领域和金融领域进行了针对性的预训练与微调。</p>
</li>
<li>
<p>Cornucopia（聚宝盆）：开源了经过中文金融知识指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过中文金融公开数据+爬取的金融数据构建指令数据集，并在此基础上对LLaMA进行了指令微调，提高了 LLaMA 在金融领域的问答效果。基于相同的数据，后期还会利用GPT3.5 API构建高质量的数据集，另在中文知识图谱-金融上进一步扩充高质量的指令数据集。</p>
</li>
<li>
<p>BBT-FinCUGE-Applications：开源了中文金融领域开源语料库BBT-FinCorpus，中文金融领域知识增强型预训练语言模型BBT-FinT5及中文金融领域自然语言处理评测基准CFLEB。</p>
</li>
</ul>
<h4 id="法律领域大模型">法律领域大模型</h4>
<ul>
<li>
<p>ChatLaw：由北京大学开源的大模型，主要有13B、33B。</p>
</li>
<li>
<p>LexiLaw：LexiLaw 是一个经过微调的中文法律大模型，它基于 ChatGLM-6B 架构，通过在法律领域的数据集上进行微调，使其在提供法律咨询和支持方面具备更高的性能和专业性。</p>
</li>
<li>
<p>LaWGPT：该系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基础上扩充法律领域专有词表、大规模中文法律语料预训练，增强了大模型在法律领域的基础语义理解能力。在此基础上，构造法律领域对话问答数据集、中国司法考试数据集进行指令精调，提升了模型对法律内容的理解和执行能力。</p>
</li>
<li>
<p>Lawyer LLaMA：开源了一系列法律领域的指令微调数据和基于LLaMA训练的中文法律大模型的参数。Lawyer LLaMA 首先在大规模法律语料上进行了continual pretraining。在此基础上，借助ChatGPT收集了一批对中国国家统一法律职业资格考试客观题（以下简称法考）的分析和对法律咨询的回答，利用收集到的数据对模型进行指令微调，让模型习得将法律知识应用到具体场景中的能力。</p>
</li>
</ul>
<h4 id="医疗领域大模型">医疗领域大模型</h4>
<ul>
<li>
<p>DoctorGLM：基于 ChatGLM-6B的中文问诊模型，通过中文医疗对话数据集进行微调，实现了包括lora、p-tuningv2等微调及部署。</p>
</li>
<li>
<p>BenTsao：源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。</p>
</li>
<li>
<p>BianQue：一个经过指令与多轮问询对话联合微调的医疗对话大模型，基于ClueAI/ChatYuan-large-v2作为底座，使用中文医疗问答指令与多轮问询对话混合数据集进行微调。</p>
</li>
<li>
<p>HuatuoGPT：开源了经过中文医学指令精调/指令微调(Instruct-tuning)的一个GPT-like模型。</p>
</li>
<li>
<p>Med-ChatGLM：基于中文医学知识的ChatGLM模型微调，微调数据与BenTsao相同。</p>
</li>
<li>
<p>QiZhenGPT：该项目利用启真医学知识库构建的中文医学指令数据集，并基于此在LLaMA-7B模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。</p>
</li>
<li>
<p>XrayGLM：该项目为促进中文领域医学多模态大模型的研究发展，发布了XrayGLM数据集及模型，其在医学影像诊断和多轮交互对话上显示出了非凡的潜力。</p>
</li>
<li>
<p>MedicalGPT：训练医疗大模型，实现包括二次预训练、有监督微调、奖励建模、强化学习训练。发布中文医疗LoRA模型shibing624/ziya-llama-13b-medical-lora，基于Ziya-LLaMA-13B-v1模型，SFT微调了一版医疗模型，医疗问答效果有提升，发布微调后的LoRA权重。</p>
</li>
</ul>
<h4 id="教育领域大模型">教育领域大模型</h4>
<ul>
<li>
<p>桃李（Taoli）：一个在国际中文教育领域数据上进行了额外训练的模型。项目基于目前国际中文教育领域流通的500余册国际中文教育教材与教辅书、汉语水平考试试题以及汉语学习者词典等，构建了国际中文教育资源库，构造了共计 88000 条的高质量国际中文教育问答数据集，并利用收集到的数据对模型进行指令微调，让模型习得将知识应用到具体场景中的能力。</p>
</li>
<li>
<p>EduChat：该项目华东师范大学计算机科学与技术学院的EduNLP团队研发，主要研究以预训练大模型为基底的教育对话大模型相关技术，融合多样化的教育垂直领域数据，辅以指令微调、价值观对齐等方法，提供教育场景下自动出题、作业批改、情感支持、课程辅导、高考咨询等丰富功能，服务于广大老师、学生和家长群体，助力实现因材施教、公平公正、富有温度的智能教育。</p>
</li>
</ul>
<h4 id="数学领域大模型">数学领域大模型</h4>
<ul>
<li>chatglm-maths：基于chatglm-6b微调/LORA/PPO/推理的数学题解题大模型, 样本为自动生成的整数/小数加减乘除运算, 可gpu/cpu部署，开源了训练数据集等。</li>
</ul>
<h4 id="rlhf人工反馈强化学习-新增20230703">RLHF(人工反馈强化学习)-新增（2023.07.03）</h4>
<p>根据 OpenAI 之前做的一些实验，可以看到使用了 PPO（近端策略优化）算法的 RLHF 模型整体上都更好一些。当把结果提供给人类时，相比于 SFT 模型和通过 prompt 化身为助理的基础模型，人类也基本更喜欢来自 RLHF 模型的 token。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/mmbiz_png/J0mLianhFicBGUvRuBHyDZSjCRF5dwbmJP3016nWVHBeacwkezf9yZdEtqytDGIvcslsCo4yicPwh1ia5ia209C66BA/640?wx_fmt=png" alt="">image.png</p>
<p>那 RLHF 为什么能让模型更好呢？目前 AI 研究界还没有找到一个得到大家认可的理论，一种可能<strong>与比较和生成的计算难度之间的不对称性有关</strong> 。</p>
<p>举个例子说明一下：假设我们要让一个模型写一首关于回形针的俳句。如果你是一位正努力创建训练数据的合同工，正在为 SFT 模型收集数据。那么你该怎样写出一首关于回形针的好俳句呢？而你可能并不是一位优秀的俳句诗人。但是，如果给你几首俳句，你却有能力辨别它们中哪首更好一些。也就是说，比起创建一个好样本，判断哪个样本更好是简单得多的任务。因此，这种不对称性可能使得比较是一种更好的方法，能更好地利用人类的判断来创造出好一些的模型。</p>
<p>但目前来看，RLHF 并不总是会为基石模型带来提升。在某些情况下，RLHF 模型会失去一些熵，也就是说它们会输出更加单调、变化更少的结果。而基础模型的熵更高，可以输出更加多样化的结果。</p>
<h4 id="rlhf开源工具">RLHF开源工具</h4>
<p>下面是目前开源的一些RLHF工具：</p>
<ul>
<li>
<p>DeepSpeed Chat：基于Opt系列模型进行示例。</p>
</li>
<li>
<p>ColossalChat：基于LLaMA系列模型进行示例。</p>
</li>
</ul>
<h4 id="分布式并行及显存优化技术">分布式并行及显存优化技术</h4>
<p><strong>并行技术</strong> ：</p>
<ul>
<li>
<p>数据并行（如：PyTorch DDP）</p>
</li>
<li>
<p>模型/张量并行（如：Megatron-LM（1D）、Colossal-AI（2D、2.5D、3D））</p>
</li>
<li>
<p>流水线并行（如：GPipe、PipeDream、PipeDream-2BW、PipeDream Flush（1F1B））</p>
</li>
<li>
<p>多维混合并行（如：3D并行（数据并行、模型并行、流水线并行））</p>
</li>
<li>
<p>自动并行（如：Alpa（自动算子内/算子间并行））</p>
</li>
<li>
<p>优化器相关的并行（如：ZeRO（<strong>零冗余优化器</strong> ，在执行的逻辑上是数据并行，但可以达到模型并行的显存优化效果）、PyTorch FSDP）</p>
</li>
</ul>
<p><strong>显存优化技术</strong> ：</p>
<ul>
<li>
<p>重计算(Recomputation)：Activation checkpointing(Gradient checkpointing)，本质上是一种用时间换空间的策略。</p>
</li>
<li>
<p>卸载（Offload）技术：一种用通信换显存的方法，简单来说就是让模型参数、激活值等在CPU内存和GPU显存之间左右横跳。如：ZeRO-Offload、ZeRO-Infinity等。</p>
</li>
<li>
<p>混合精度（BF16/FP16）：降低训练显存的消耗，还能将训练速度提升2-4倍。</p>
<ul>
<li>
<p>BF16 计算时可避免计算溢出，出现Inf case。</p>
</li>
<li>
<p>FP16 在输入数据超过65506 时，计算结果溢出，出现Inf case。</p>
</li>
</ul>
</li>
</ul>
<h4 id="分布式训练框架">分布式训练框架</h4>
<p><strong>如何选择一款分布式训练框架</strong> ？</p>
<p>*<strong>训练成本</strong> ：不同的训练工具，训练同样的大模型，成本是不一样的。对于大模型，训练一次动辄上百万/千万美元的费用。合适的成本始终是正确的选择。</p>
<p>*<strong>训练类型</strong> ：是否支持数据并行、张量并行、流水线并行、多维混合并行、自动并行等</p>
<p>*<strong>效率</strong> ：将普通模型训练代码变为分布式训练所需编写代码的行数，我们希望越少越好。</p>
<p>*<strong>灵活性</strong> ：你选择的框架是否可以跨不同平台使用？</p>
<p><strong>常见的分布式训练框架</strong> ：</p>
<ul>
<li>
<p>第一类：深度学习框架自带的分布式训练功能。如：TensorFlow、PyTorch、MindSpore、Oneflow、PaddlePaddle等。</p>
</li>
<li>
<p>第二类：基于现有的深度学习框架（如：PyTorch、Flax）进行扩展和优化，从而进行分布式训练。如：Megatron-LM（张量并行）、DeepSpeed（Zero-DP）、Colossal-AI（高维模型并行，如2D、2.5D、3D）、Alpa（自动并行）等</p>
</li>
</ul>
<p><strong>目前训练超大规模语言模型主要有两条技术路线</strong> ：</p>
<ol>
<li>
<p>TPU + XLA + TensorFlow/JAX ：由Google主导，由于TPU和自家云平台GCP深度绑定。</p>
</li>
<li>
<p>GPU + PyTorch + Megatron-LM + DeepSpeed ：由NVIDIA、Meta、MicroSoft大厂加持，社区氛围活跃，也更受到大家欢迎。</p>
</li>
</ol>
<p>对于国内来说，华为昇腾在打造 AI 全栈软硬件平台（昇腾NPU+CANN+MindSpore+MindFormers）。不过目前整个生态相对前两者，还差很远。</p>
<h4 id="参数高效微调peft技术">参数高效微调（PEFT）技术</h4>
<p>在面对特定的下游任务时，如果进行Full FineTuning（即对预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</p>
<p>PEFT技术旨在<strong>通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本</strong> 。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。因此，PEFT技术可以在提高模型效果的同时，大大缩短模型训练时间和计算成本，让更多人能够参与到深度学习研究中来。除此之外，FEFT可以缓解全量微调带来灾难性遗忘的问题。</p>
<p>*<strong>Prefix Tuning</strong> ：与full fine-tuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。 同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。</p>
<p>*<strong>Prompt Tuning</strong> ：该方法可以看作是Prefix Tuning的简化版本，只在输入层加入prompt tokens，并不需要加入MLP进行调整来解决难训练的问题。随着预训练模型参数量的增加，Prompt Tuning的方法会逼近fine-tuning的结果。</p>
<p>*<strong>P-Tuning</strong> ：该方法的提出主要是为了解决这样一个问题：大模型的Prompt构造方式严重影响下游任务的效果。P-Tuning将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对prompt embedding进行一层处理。</p>
<p>*<strong>P-Tuning v2</strong> ：让Prompt Tuning能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌Fine-tuning的结果。相比Prompt Tuning和P-tuning的方法，P-Tuning v2方法在多层加入了Prompts tokens作为输入，带来两个方面的好处：</p>
<pre><code>1. 带来更多可学习的参数（从P-tuning和Prompt Tuning的0.1%增加到0.1%-3%），同时也足够参数高效。


2. 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。
</code></pre>
<p>*<strong>Adapter Tuning</strong> ：该方法设计了Adapter结构（首先是一个down-project层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个up-project结构将低维特征映射回原来的高维特征；同时也设计了skip-connection结构，确保了在最差的情况下能够退化为identity），并将其嵌入Transformer的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的Adapter结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数）。</p>
<p>*<strong>LoRA</strong> ：在涉及到矩阵相乘的模块，引入A、B这样两个低秩矩阵模块去模拟full fine-tuning的过程，相当于只对语言模型中起关键作用的低秩本质维度进行更新。</p>
<ul>
<li>QLoRA：使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。目前，训练速度较慢。</li>
</ul>
<p>*<strong>AdaLoRA</strong> ：对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。</p>
<p>&hellip;</p>
<p><strong>典型应用</strong> ：</p>
<ol>
<li>
<p>ChatGLM-Tuning ：一种平价的chatgpt实现方案，基于清华的 ChatGLM-6B + LoRA 进行finetune。</p>
</li>
<li>
<p>Alpaca-Lora：使用低秩自适应（LoRA）复现斯坦福羊驼的结果。Stanford Alpaca 是在 LLaMA 整个模型上微调，而 Alpaca-Lora 则是利用 Lora 技术，在冻结原模型 LLaMA 参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅微调的成本显著下降，还能获得和全模型微调类似的效果。</p>
</li>
<li>
<p>BLOOM-LORA：由于LLaMA的限制，我们尝试使用Alpaca-Lora重新实现BLOOM-LoRA。</p>
</li>
</ol>
<p><strong>PEFT实现</strong> ：</p>
<ol>
<li>
<p>PEFT：Huggingface推出的PEFT库。</p>
</li>
<li>
<p>unify-parameter-efficient-tuning：一个参数高效迁移学习的统一框架。</p>
</li>
</ol>
<p><strong>高效微调技术目前存在的两个问题</strong> ：</p>
<p>相比全参数微调，大部分的高效微调技术目前存在的两个问题：</p>
<ol>
<li>
<p>推理速度会变慢</p>
</li>
<li>
<p>模型精度会变差</p>
</li>
</ol>
<h4 id="影响大模型性能的主要因素">影响大模型性能的主要因素</h4>
<p>OpenAI的论文<strong>Scaling Laws for Neural Language Models</strong> 中列举了影响模型性能最大的三个因素：<strong>计算量</strong> 、<strong>数据集大小</strong> 、<strong>模型参数量</strong> 。也就是说，当其他因素不成为瓶颈时，计算量、数据集大小、模型参数量这3个因素中的单个因素指数增加时，loss会线性的下降。</p>
<p>除了以上的因素之外，还有一个比较大的影响因素就是<strong>数据质量</strong> 。在微软的论文<strong>Instruction Tuning with GPT-4</strong> 中指出，同样基于LLaMA模型，使用GPT3和GPT4产生的数据，对模型进行Instruction Turing，可以看到GPT4的数据微调过的模型效果远远好于GPT3数据微调的模型，可见数据质量带来的影响。同样的，Vicuna（7B/13B）的Instruction Turing中，也对shareGPT的数据做了很细致的清洗工作。</p>
<h4 id="衡量大模型水平">衡量大模型水平</h4>
<p>要评估一个大型语言模型的水平，可以从以下几个维度提出具有代表性的问题。</p>
<p>*<strong>理解能力</strong> ：提出一些需要深入理解文本的问题，看模型是否能准确回答。</p>
<p>*<strong>语言生成能力</strong> ：让模型生成一段有关特定主题的文章或故事，评估其生成的文本在结构、逻辑和语法等方面的质量。</p>
<p>*<strong>知识面广度</strong> ：请模型回答关于不同主题的问题，以测试其对不同领域的知识掌握程度。这可以是关于科学、历史、文学、体育或其他领域的问题。一个优秀的大语言模型应该可以回答各种领域的问题，并且准确性和深度都很高。</p>
<p>*<strong>适应性</strong> ：让模型处理各种不同类型的任务，例如：写作、翻译、编程等，看它是否能灵活应对。</p>
<p>*<strong>长文本理解</strong> ：提出一些需要处理长文本的问题，例如：提供一篇文章，让模型总结出文章的要点，或者请模型创作一个故事或一篇文章，让其有一个完整的情节，并且不要出现明显的逻辑矛盾或故事结构上的错误。一个好的大语言模型应该能够以一个连贯的方式讲述一个故事，让读者沉浸其中。</p>
<p>*<strong>长文本生成</strong> ：请模型创作一个故事或一篇文章，让其有一个完整的情节，并且不要出现明显的逻辑矛盾或故事结构上的错误。一个好的大语言模型应该能够以一个连贯的方式讲述一个故事，让读者沉浸其中。</p>
<p>*<strong>多样性</strong> ：提出一个问题，让模型给出多个不同的答案或解决方案，测试模型的创造力和多样性。</p>
<p>*<strong>情感分析和推断</strong> ：提供一段对话或文本，让模型分析其中的情感和态度，或者推断角色间的关系。</p>
<p>*<strong>情感表达</strong> ：请模型生成带有情感色彩的文本，如描述某个场景或事件的情感、描述一个人物的情感状态等。一个优秀的大语言模型应该能够准确地捕捉情感，将其表达出来。</p>
<p>*<strong>逻辑推理能力</strong> ：请模型回答需要进行推理或逻辑分析的问题，如概率或逻辑推理等。这可以帮助判断模型对推理和逻辑思考的能力，以及其在处理逻辑问题方面的准确性。例如：“所有的动物都会呼吸。狗是一种动物。那么狗会呼吸吗？”</p>
<p>*<strong>问题解决能力</strong> ：提出实际问题，例如：数学题、编程问题等，看模型是否能给出正确的解答。</p>
<p>*<strong>道德和伦理</strong> ：测试模型在处理有关道德和伦理问题时的表现，例如：“在什么情况下撒谎是可以接受的？”</p>
<p>*<strong>对话和聊天</strong> ：请模型进行对话，以测试其对自然语言处理的掌握程度和能力。一个优秀的大语言模型应该能够准确地回答问题，并且能够理解人类的语言表达方式。</p>
<h4 id="heading"></h4>
<p><strong>大模型评估方法</strong> ：</p>
<p><strong>人工评估</strong> ：LIMA、Phoenix</p>
<p><strong>使用 GPT-4 的反馈进行自动评估</strong> ：Vicuna、Phoenix、Chimera、BELLE</p>
<p><strong>指标评估</strong> （BLEU-4、ROUGE分数）：ChatGLM-6B；对于像ROUGE-L分数的指标评估，有些地方称其为非自然指令评估（Unnatural Instruction Evaluation）。</p>
<p><strong>Chatbot Arena</strong> ：目前用来衡量一个模型好不好的东西基本都是基于一些学术的benchmark，比如在一个某个NLP任务上构建一个测试数据集，然后看测试数据集上准确率多少。然而，这些学术benchmark（如HELM）在大模型和聊天机器人上就不好用了。其原因在于：</p>
<ul>
<li>
<p>由于评判聊天机器人聊得好不好这件事是非常主观的，因此，现有的方法很难对其进行衡量。</p>
</li>
<li>
<p>这些大模型在训练的时候就几乎把整个互联网的数据都扫了一个遍，因此，很难保证测试用的数据集没有被看到过。甚至更进一步，用测试集直接对模型进行「特训」，如此一来表现必然更好。</p>
</li>
<li>
<p>理论上我们可以和聊天机器人聊任何事情，但很多话题或者任务在现存的benchmark里面根本就不存在。</p>
</li>
</ul>
<p>因此，Chatbot Arena 的做法是放弃benchmark，通过对抗，实时聊天，两两比对人工进行打分，采用elo分数进行评测。</p>
<p><strong>大模型评估工具</strong> ：</p>
<ul>
<li>
<p>OpenAI evals：OpenAI的自动化评估脚本，核心思路就是通过写prompt模版来自动化评估。</p>
</li>
<li>
<p>PandaLM：其是直接训练了一个自动化打分模型，0,1,2三分制用模型对两个候选模型进行打分。</p>
</li>
</ul>
<h4 id="大模型推理加速">大模型推理加速</h4>
<p>模型推理作为模型投产的最后一公里，需要确保模型精度的同时追求极致的推理性能。相比传统模型来说，大模型面临着更多的挑战。</p>
<p>当前优化模型最主要技术手段概括来说有以下三个层面：</p>
<ul>
<li>
<p>算法层面：蒸馏、量化</p>
</li>
<li>
<p>软件层面：计算图优化、模型编译</p>
</li>
<li>
<p>硬件层面：FP8（NVIDIA H系列GPU开始支持FP8，兼有fp16的稳定性和int8的速度）</p>
</li>
</ul>
<p><strong>推理加速框架</strong> ：</p>
<p>*<strong>FasterTransformer</strong> ：英伟达推出的FasterTransformer不修改模型架构而是在计算加速层面优化 Transformer 的 encoder 和 decoder 模块。具体包括如下：</p>
<pre><code>* 尽可能多地融合除了 GEMM 以外的操作


* 支持 FP16、INT8、FP8


* 移除 encoder 输入中无用的 padding 来减少计算开销
</code></pre>
<p>*<strong>TurboTransformers</strong> ：腾讯推出的 TurboTransformers 由 computation runtime 及 serving framework 组成。加速推理框架适用于 CPU 和 GPU，最重要的是，它可以无需预处理便可处理变长的输入序列。具体包括如下：</p>
<pre><code>* 与 FasterTransformer 类似，它融合了除 GEMM 之外的操作以减少计算量


* smart batching，对于一个 batch 内不同长度的序列，它也最小化了 zero-padding 开销


* 对 LayerNorm 和 Softmax 进行批处理，使它们更适合并行计算


* 引入了模型感知分配器，以确保在可变长度请求服务期间内存占用较小
</code></pre>
<h4 id="ai-集群网络通信-新增20230630">AI 集群网络通信-新增（2023.06.30）</h4>
<h4 id="通信硬件">通信硬件</h4>
<p>机器内通信：</p>
<ul>
<li>
<p>共享内存，比如：CPU与CPU之间的通信可以通过共享内存。</p>
</li>
<li>
<p>PCIe，通常是CPU与GPU之间的通信，也可以用于GPU与GPU之间的通信。</p>
</li>
<li>
<p>NVLink（直连模式），通常是GPU与GPU之间的通信，也可以用于CPU与GPU之间的通信。</p>
</li>
</ul>
<p>机器间通信：</p>
<ul>
<li>
<p>TCP/IP网络</p>
</li>
<li>
<p>RDMA：远程直接内存访问，目前主要有如下三种技术：</p>
<ul>
<li>
<p>InfiniBand</p>
</li>
<li>
<p>iWarp</p>
</li>
<li>
<p>RoCE v2</p>
</li>
</ul>
</li>
</ul>
<h4 id="通信软件">通信软件</h4>
<p>下面是一些常见的网络通信库：</p>
<ul>
<li>
<p>Gloo： Facebook 开源的一套集体通信库，提供了对机器学习中有用的一些集合通信算法。</p>
</li>
<li>
<p>NCCL：英伟达基于 NVIDIA-GPU 的一套开源的集合通信库。</p>
</li>
<li>
<p>OpenMPI：一个开源 MPI（消息传递接口 ）的实现，由学术，研究和行业合作伙伴联盟开发和维护。</p>
</li>
<li>
<p>HCCL：华为开发的网络通信库。</p>
</li>
</ul>
<h4 id="通信网络监控">通信网络监控</h4>
<ul>
<li>
<p>nvbandwidth：用于测量 NVIDIA GPU 带宽的工具。</p>
</li>
<li>
<p>DCGM：一个用于收集telemetry数据和测量 NVIDIA GPU 运行状况。</p>
</li>
</ul>
<h4 id="大模型生态相关技术-新增20230630">大模型生态相关技术-新增（2023.06.30）</h4>
<p>大模型是基座，要想让其变成一款产品，我们还需要一些其他相关技术：</p>
<h4 id="llm-应用开发工具">LLM 应用开发工具</h4>
<ul>
<li>
<p>langchain：一个用于构建基于大型语言模型（LLM）的应用程序的库。它可以帮助开发者将LLM 与其他计算或知识源结合起来，创建更强大的应用程序。</p>
</li>
<li>
<p>llama-index：一个将大语言模型和外部数据连接在一起的工具。</p>
</li>
<li>
<p>gpt-cache：LLM 语义缓存层（caching layer），它采用语义缓存（semantic cache）技术，能够存储 LLM 响应，从而显著减少检索数据所需的时间、降低 API 调用开销、提升应用可扩展性。</p>
</li>
</ul>
<h4 id="向量数据库">向量数据库</h4>
<ul>
<li>
<p>Pinecone</p>
</li>
<li>
<p>Milvus</p>
</li>
<li>
<p>Vespa</p>
</li>
<li>
<p>Weaviate</p>
</li>
</ul>
<p>总的来说，如果想快速验证，Pinecone 是个不错的选择。如果想拥有更灵活的查询方式，可以考虑 Vespa 或 Weaviate.如果需要更好的可扩展性和可靠性，那么经过大客户验证的 Vespa 或 Milvus 可能是不错的选择。</p>
<h4 id="经验与教训">经验与教训</h4>
<p><strong>经验</strong> ：</p>
<ul>
<li>
<p>对于同一模型，选择不同的训练框架，对于资源的消耗情况可能存在显著差异（比如使用Huggingface Transformers和DeepSpeed训练OPT-30相对于使用Alpa对于资源的消耗会低不少）。</p>
</li>
<li>
<p>进行大模型模型训练时，先使用小规模模型（如：OPT-125m/2.7b）进行尝试，然后再进行大规模模型（如：OPT-13b/30b&hellip;）的尝试，便于出现问题时进行排查。目前来看，业界也是基于相对较小规模参数的模型（6B/7B/13B）进行的优化，同时，13B模型经过指令精调之后的模型效果已经能够到达GPT4的90%的效果。</p>
</li>
<li>
<p>对于一些国产AI加速卡，目前来说，坑还比较多，如果时间不是时间非常充裕，还是尽量选择Nvidia的AI加速卡。</p>
</li>
</ul>
<p><strong>教训</strong> ：</p>
<ul>
<li>
<p>针对已有的环境进行分布式训练环境搭建时，一定要注意之前环境的python、pip、virtualenv、setuptools的版本。不然创建的虚拟环境即使指定对了Python版本，也可能会遇到很多安装依赖库的问题（GPU服务器能够访问外网的情况下，建议使用Docker相对来说更方便）。</p>
</li>
<li>
<p>遇到需要升级GLIBC等底层库需要升级的提示时，一定要慎重，不要轻易升级，否则，可能会造成系统宕机或很多命令无法操作等情况。</p>
</li>
</ul>
<h4 id="大模型实践文章">大模型实践文章</h4>
<p>下面是最近大模型实践过程中的一些文章，文档及配套代码均放置在GitHub：llm-action。</p>
<p><strong>LLM训练</strong> :</p>
<p>LLM预训练/微调/RLHF&hellip;参数教程代码</p>
<p>Alpaca
full fine-turning
7B
从0到1复现斯坦福羊驼（Stanford Alpaca 7B）
N/A</p>
<p>Alpaca
lora
7B</p>
<ol>
<li>足够惊艳，使用Alpaca-Lora基于LLaMA(7B)二十分钟完成微调，效果比肩斯坦福羊驼</li>
<li>使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理
配套代码</li>
</ol>
<p>BELLE(LLaMA-7B/Bloomz-7B1-mt)
full fine-turning
7B</p>
<ol>
<li>基于LLaMA-7B/Bloomz-7B1-mt复现开源中文对话大模型BELLE及GPTQ量化</li>
<li>BELLE(LLaMA-7B/Bloomz-7B1-mt)大模型使用GPTQ量化后推理性能测试
N/A</li>
</ol>
<p>ChatGLM
lora
6B
从0到1基于ChatGLM-6B使用LoRA进行参数高效微调
N/A</p>
<p>ChatGLM
full fine-turning/P-Tuning v2
6B
使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调
N/A</p>
<p>Vicuna
full fine-turning
7B
大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼
N/A</p>
<p>OPT
RLHF
N/A</p>
<ol>
<li>一键式 RLHF 训练 DeepSpeed Chat（一）：理论篇</li>
<li>一键式 RLHF 训练 DeepSpeed Chat（二）：实践篇
N/A</li>
</ol>
<p>MiniGPT-4
full fine-turning
7B
大杀器，多模态大模型MiniGPT-4入坑指南
N/A</p>
<p>Chinese-LLaMA-Alpaca
lora（预训练+微调）
7B
使用 LoRA 技术对 LLaMA 65B 大模型进行微调及推理
配套代码</p>
<p><strong>LLM推理</strong> :</p>
<ul>
<li>
<p>大模型的好伙伴，浅析推理加速引擎FasterTransformer</p>
</li>
<li>
<p>模型推理服务化框架Triton保姆式教程（一）：快速入门</p>
</li>
<li>
<p>模型推理服务化框架Triton保姆式教程（二）：架构解析</p>
</li>
<li>
<p>模型推理服务化框架Triton保姆式教程（三）：开发实践</p>
</li>
</ul>
<p><strong>LLM微调技术</strong> ：</p>
<p>对于普通大众来说，进行大模型的预训练或者全量微调遥不可及。由此，催生了各种参数高效微调技术，让科研人员或者普通开发者有机会尝试微调大模型。</p>
<p>因此，该技术值得我们进行深入分析其背后的机理，本系列大体分七篇文章进行讲解。</p>
<ul>
<li>
<p>大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（四）-Adapter Tuning及其变体</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（五）-LoRA、AdaLoRA、QLoRA</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（六）-MAM Adapter、UniPELT</p>
</li>
<li>
<p>大模型参数高效微调技术原理综述（七）-最佳实践、总结</p>
</li>
</ul>
<h4 id="大模型技术交流群">大模型技术交流群</h4>
<p>另外，我创建了一个大模型学习交流群，供大家一起学习交流大模型相关的最新技术，可加我微信（<strong>lgd215666835</strong> ）进群（加微信请备注来意，如：进大模型学习交流群）。</p>
<h4 id="结语">结语</h4>
<p>实践出真知，以上是这段时间进行大模型实践的一点点总结，写的有一些主观和片面，后续会持续更新自己研究大模型获得的一些认知和实践经验，希望能够帮助大家，欢迎点赞收藏加关注。</p>
<p>更多AI工具，参考<a href="https://ai123.869hr.uk/">Github-AI123</a>，<a href="https://ai123.869hr.uk/">国内AI123</a></p>



          </div>



<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


