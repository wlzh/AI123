

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <meta name="theme-color" content="#f9f9f9" />

	<title>这个Transformer速查宝典，是真的全！ 作者： DASOU 来源： DASOU 各位好，我是DASOU； 大模型如火如荼，最近国外有个网友最近写了一个关于Transformer 的综述文章(注: kipply&rsquo;s blog)，质量还挺高的。建议大家收藏阅读一下。 这个综述涵盖了 21 种Trm架构大模型、11 种架构变化、7 种预训练后  | AI123| ai工具网址导航,ai最新产品</title>
	<link rel="shortcut icon" href="/assets/images/favicon.png" />
    <meta name="keywords" content="chatgpt,AI,AI聊天,AI文本生成,AI绘画,AI编程,AI电商" />
    <meta name="description" content="AI123 网址导航 | 免费chatgpt 汇集各类先进的人工智能产品，旨在帮助用户更快速地了解和使用这些产品,轻松地浏览不同领域的AI产品，包括语音识别、图像处理、自然语言处理。" />
    
    <meta name="baidu-site-verification" content="codeva-cCAOSG8MBO" />
    
    <link rel="stylesheet" id="block-library-css"
        href="/assets/css/block-library.min-5.6.2.css" type="text/css" media="all" />
    <link rel="stylesheet" id="iconfont-css" href="/assets/css/iconfont-3.03029.1.css"
        type="text/css" media="all" />

    
    <link href="/scss/style.min.css" rel="stylesheet" />
    
		    <link rel="stylesheet" id="iowen-css" href="/assets/css/style-3.03029.1.css"
        type="text/css" media="all" />
    <link rel="stylesheet" id="custom-css" href="/assets/css/custom-style.css"
        type="text/css" media="all" />
		
		<link rel="stylesheet" href=/plugins/font-awesome/css/font-awesome.min.css />


    <link rel="stylesheet" id="fortawesome-css" href="/assets/fontawesome-5.15.4/css/all.min.css" type="text/css" />


    <script type="text/javascript" src="/assets/js/jquery.min-3.2.1.js" id="jquery-js"></script>
    <script type="text/javascript" src="/assets/js/content-search.js"  id="content-search-js"></script>

    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2073588164294660"
     crossorigin="anonymous"></script>

	
    <script>
        

		var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8450bc732b2a86f7e4aec4ebd9fd8252";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

        
    </script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7071W80M2K"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-7071W80M2K');
    </script>

</head>


    <div class="page-container">
	
	<div id="sidebar" class="sticky sidebar-nav fade animate-nav" style="width: 170px">
        
            <div class="modal-dialog h-100 sidebar-nav-inner">
                <div class="sidebar-logo border-bottom border-color">
                    
                    <div class="logo overflow-hidden">
                        <a href="https://ai123.869hr.uk/" class="logo-expanded">
                            <img src="/assets/images/bt8-expand-light.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt8-expand-dark.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                        <a href="https://ai123.869hr.uk/" class="logo-collapsed">
                            <img src="/assets/images/bt.png" height="40" class="logo-light"
                                alt="AI123| ai工具网址导航,ai最新产品">
                            <img src="/assets/images/bt.png" height="40" class="logo-dark d-none"
                                alt="AI123| ai工具网址导航,ai最新产品">
                        </a>
                    </div>
                    
                </div>
                <div class="sidebar-menu flex-fill">
                    <div class="sidebar-scroll">
                        <div class="sidebar-menu-inner">
                            <ul>
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#00834a9dd147b04c5d53d4368cdb0b57" class="smooth">
                                            <i class="fas fa-sun fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>本月热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db0311e7ecfedd24d157f0ceb4a0897f" class="smooth">
                                            <i class="fas fa-star-and-crescent fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>热门网站</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#21b5cbb2c769010fec3ce029a5f8a4a3" class="smooth">
                                            <i class="far fa-star fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>国内热门</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#8310718935e8ec25ce0350de01e3f7dc" class="smooth">
                                            <i class="fas fa-phone fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>对话工具</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#d58e850d9115797306c2edf61ac6ddd8" class="smooth">
                                            <i class="fas fa-newspaper fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>写作</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2a7418a5f8f1ca4e054364a9300657df" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#7808a68ee1b34dab43011429a12de19e" class="smooth">
                                            <i class="fas fa-image fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>图像处理</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#6729afc51f5ac49a828812fa0eb0c82f" class="smooth">
                                            <i class="fas fa-video fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音视频</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#e5ce844860451fff3faf3d8f8894971d" class="smooth">
                                            <i class="fas fa-music fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>音乐生成</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#db53804b7d726967c58fcc8c9ca03d27" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>办公</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#47b7af9547e034d28fe6f6d439968ac8" class="smooth">
                                            <i class="fas fa-copy fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>提示词</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#41282bf95e43c64d579757573a03cdde" class="smooth">
                                            <i class="fas fa-code fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>编程</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#fd71852fd52d5e18ef4f9a252f1eac58" class="smooth">
                                            <i class="fas fa-search fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>AI搜索</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#81b1637fbe47625dbdf2094acd3b6683" class="smooth">
                                            <i class="fas fa-language fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>文本翻译</span>
                                        </a>
                                    </li>
                                    
                                
                                    
                                    <li class="sidebar-item">
                                        <a href="/#2e9ba3fa6e1ed0e9311b3e97f97f9a40" class="smooth">
                                            <i class="fas fa-book fa-lg fa-lg icon-fw icon-lg mr-2"></i>
                                            <span>学习网站</span>
                                        </a>
                                    </li>
                                    
                                
                            </ul>           
                        </div>
                    </div>
                </div>
                <div class="border-top py-2 border-color">
                    <div class="flex-bottom">
                        <ul>
			    <li id="menu-item-212"
                                 class="menu-item menu-item-type-custom menu-item-object-custom menu-item-212 sidebar-item">
                                 <a href="#friendlink" class="smooth">
                                     <i class="fab fa-staylinked icon-fw icon-lg mr-2"></i>
                                     <span>友情链接</span>
                                 </a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>


<div class="flex-fill grid-bg">
    <div class="big-header-banner">
        <div id="header" class="page-header sticky">
            <div class="navbar navbar-expand-md">
                <div class="container-fluid p-0">

                    <a href="" class="navbar-brand d-md-none" title="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-light"
                            alt="AI123| ai工具网址导航,ai最新产品">
                        <img src="/assets/images/bt.png" class="logo-dark d-none"
                            alt="AI123| ai工具网址导航,ai最新产品">
                    </a>

                    <div class="collapse navbar-collapse order-2 order-md-1">
                        <div class="header-mini-btn">
                            <label>
                                <input id="mini-button" type="checkbox">
                                <svg viewbox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
                                    <path class="line--1" d="M0 40h62c18 0 18-20-17 5L31 55"></path>
                                    <path class="line--2" d="M0 50h80"></path>
                                    <path class="line--3" d="M0 60h62c18 0 18 20-17-5L31 45"></path>
                                </svg>
                            </label>

                        </div>

                        <ul class="navbar-nav site-menu" style="margin-right: 16px;">
                        
			<li >
				<a href="/">
                                    <i class="fa fa-home fa-lg mr-2"></i>
                                    <span>首页</span>
                                </a>
				<ul class="sub-menu">
				
				</ul>
			    </li>
			
			</ul>

                        
                        <div class="rounded-circle weather">
                            <div id="he-plugin-simple" style="display: contents;"></div>
                            <script>WIDGET = {
                                    CONFIG: {
                                        "modules": "01234",
                                        "background": 5,
                                        "tmpColor": "008000",
                                        "tmpSize": 14,
                                        "cityColor": "008000",
                                        "citySize": 14,
                                        "aqiColor": "#008000",
                                        "aqiSize": 14,
                                        "weatherIconSize": 24,
                                        "alertIconSize": 18,
                                        "padding": "10px 10px 10px 10px",
                                        "shadow": "1",
                                        "language": "auto",
                                        "borderRadius": 5,
                                        "fixed": "false",
                                        "vertical": "middle",
                                        "horizontal": "left",
                                        "key": "085791e805a24491b43b06cf58ab31e7"
                                    }
                                }
                            </script>
                            <script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script>
                        </div>
                        
                    </div>

                    <ul class="nav navbar-menu text-xs order-1 order-md-2">
                        
                        
                        <li class="nav-item mr-3 mr-lg-0 d-none d-lg-block">
                            <script>
                                fetch('https://v1.hitokoto.cn')
                                    .then(response => response.json())
                                    .then(data => {
                                    const hitokoto = document.getElementById('hitokoto_text')
                                    hitokoto.href = 'https://hitokoto.cn/?uuid=' + data.uuid
                                    hitokoto.innerText = data.hitokoto
                                    })
                                    .catch(console.error)
                            </script>                           
                            <div id="hitokoto"><a href="#" target="_blank" id="hitokoto_text">疏影横斜水清浅，暗香浮动月黄昏。</a></div>
                        </li>
                        
                        
                        <li class="nav-search ml-3 ml-md-4">
                            <a href="javascript:" data-toggle="modal" data-target="#search-modal"><i
                                    class="iconfont icon-search icon-2x"></i></a>
                        </li>
                        <li class="nav-item d-md-none mobile-menu ml-3 ml-md-4">
                            <a href="javascript:" id="sidebar-switch" data-toggle="modal"
                                data-target="#sidebar"><i class="iconfont icon-classification icon-2x"></i></a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
        <div class="placeholder" style="height:74px"></div>
    </div>




<body class="page-body boxed-container  io-grey-mode">
    <main role="main" class="flex-shrink-0">
    <div class="container">
        
        <div class="content">
            <style>
    body{
	    background: #f9f9f9;
	}

    h1, h2, h3, h4, h5, h6 {
        margin-top: 1.5rem;
        margin-bottom: 1.5rem;
    }


 
@media (min-width: 1000px) {
  .container, .container-sm {
    max-width: 800px;
  }
}

</style>

<div class="featured-post-content">

    <a href="/digest/" class="featured-post-title">
       AI 文摘
    </a>

</div>

<section class="blog-single">
  <div class="container">
    <div class="row">

      <div class="col-lg-12 order-1 order-lg-2">
        <article class="single-blog">
          <p class="title">这个Transformer速查宝典，是真的全！</p>
            <br/>
          <ul class="meta">
            <li>
              By <a href=https://ai123.869hr.uk/about>AI123</a>
            </li>
            <li>
              <i class="fa fa-clock-o"></i>
              August 14, 2023 - 2 min read
            </li>
          </ul>

          <div class="_1NCGf">
              <img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp3baarEA95h9uPDKCuSjZjDb1RBMO1d1mdQYEKYTfXzOF3h42XVgibVA/640?wx_fmt=png" width="640" >
          </div>
            <br>
            <br>
            <br>
          
          <div class="single-blog-content">
            <p>作者： DASOU  来源： <a href="https://mp.weixin.qq.com/s/Xv3ToptwFmSlw8nEfj7P9w">DASOU</a></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/LU88NSfAnCxGjIhnWpTYiasBLBZIDQiadSYT0ycu8mETbEN0urIEv9QBjedobTVbNOrUvXeyJza77Ca1OLHViak6A/640?wx_fmt=png" alt=""></p>
<p>各位好，我是DASOU；</p>
<p>大模型如火如荼，最近国外有个网友最近写了一个关于Transformer 的综述文章(注: kipply&rsquo;s blog)，质量还挺高的。建议大家收藏阅读一下。</p>
<p><strong>这个综述涵盖了 21 种Trm架构大模型、11 种架构变化、7 种预训练后处理技术和 3 种训练技术（还有 5 种不属于以上技术的东西）</strong> 。</p>
<p>模型包括 GPT-3、GPT-4、Gopher、AlphaCode、RETRO、GPT-3.5、Chinchilla、Flamingo 等。一些重要的架构变化包括多查询注意力、稀疏注意力、混合专家等。同时还介绍了 RLHF、CAI、Minerva 等预训练后处理技术以及超参。</p>
<p>所有内容均按照重要性和独特性进行排序，并将链接附在下方。</p>
<blockquote>
<p>以下翻译内容源自机器之心</p>
</blockquote>
<p><strong>一、模型</strong></p>
<p>以下模型的属性若未明确说明，要么未公开，要么大致遵循标准 GPT 配置。</p>
<p><strong>1.GPT-3</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpmaG6wFBMM9M4nrBUXT2RBxsEicMYHqhfAd9WxuM7iazNhKWlteI5NeGA/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：175B 参数，96 层，12288 嵌入维度，96 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2005.14165.pdf</p>
</li>
<li>
<p>发布详情 Open AI 发布于 2020 年 5 月</p>
</li>
</ul>
<p>本文是继 GPT-2 论文（2018 及扩展定律论文后，大语言模型的一片开创性论文。以下是论文中指出有关 GPT-3 的特征。</p>
<ul>
<li>
<p>它在一个 300B token 的数据集上进行训练。该数据集主要由过滤后的 Common Crawl 以及一些书籍、网络文本构成；</p>
</li>
<li>
<p>使用了 BPE tokenizer（与 GPT-2 相同）；</p>
</li>
<li>
<p>2048 上下文长度；</p>
</li>
<li>
<p>交替使用密集和稀疏注意力层；</p>
</li>
<li>
<p>在最初的 375M toks 中，学习率升至 0.6 × 10^-4，260B toks 后余弦衰减至 10%；</p>
</li>
<li>
<p>在前 12B 个 token 中，批大小从 32k toks 上升到 3.2M toks；</p>
</li>
<li>
<p>4x MLP 投影率，如 2017 年 Transformer 论文所示；</p>
</li>
<li>
<p>50k 词汇表（vocab size）。</p>
</li>
</ul>
<p>以上的许多特征形成了一种标准配置，被后来的模型重复使用。</p>
<blockquote>
<p>在论文记录超参数的表 2.1 中有一个可能的拼写错误，其中 GPT-3 13B 被记作为具有 5140 的嵌入维度，这里应该是 5120。</p>
</blockquote>
<p><strong>2.GPT-4</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpQ0ic16qM9ltN7CfLjxxCCH6Y7icHb0rJXxnjDvfNTa8a7C5ljO0J5pQg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>报告地址：https://arxiv.org/pdf/2303.08774.pdf</p>
</li>
<li>
<p>发布详情：Open AI 2022 年 8 月对其完成预训练，发布于 2023 年 3 月。</p>
</li>
</ul>
<p>GPT-4 是 OpenAI 提供的一个模型，其架构不明（技术上类似于 Transformer）。技术报告主要包含大部分评估（结果表现良好），以及能够从较小模型精确推断出的持续扩展结果。报告还记录了提高模型安全性的措施，并演示了 GPT-4 的多模态能力，这种能力似乎是用类似于 Flamingo 的方式训练的。</p>
<p><strong>3.Gopher</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpwZnClkJ6Pvx59ngcn4ZOSxCVaNOStFWCcl2T0r1icoBL4iaFG8GmAKTQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：280B 参数，260B 非嵌入参数，80 层，16384 嵌入维度，128 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2112.11446.pdf</p>
</li>
<li>
<p>发布详情：DeepMind 在 2020 年底对其进行训练，发布于 2021 年 12 月。</p>
</li>
</ul>
<p>Gopher 是 DeepMind 在 2021 年发布的第一个大型语言模型。它使用 RMSNorm 而不是 LayerNorm，使用 Transformer-XL 的相对位置编码方案而不是绝对位置编码，这就是嵌入参数如此之多的原因。</p>
<p>它使用 SentencePiece 进行分词，词汇表大小为 32k，并用 300B token 进行训练，其中一半来自为 Gopher 收集的 MassiveText，以及书籍、Common Crawl、新闻和 Github。</p>
<p><strong>4.AlphaCode</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpo7eHQAlHic8e1AdCibHVMPW1jWUiaibeVbow9hALibS3EibV0QrNnUmlvLXg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：41B 参数，8 个编码器层，56 个解码器层，6144 嵌入维度</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2203.07814.pdf</p>
</li>
<li>
<p>发布详情：DeepMind 发布于 2022 年 2 月。</p>
</li>
</ul>
<p>AlphaCode 是在 715GB（967B token）代码基础上训练出来的模型，可以用于解决编程竞赛问题。它是本文中唯一采用解码器 - 编码器架构的模型。它将编程竞赛题视为一项翻译任务（问题陈述 → 解决方案），以获得双向性。它在编码器中使用 1536 个 token，在解码器中使用 768 个 token。使用多查询注意力，并在推理时生成数千个样本，然后选择一个解决方案子集进行提交。</p>
<p><strong>5.RETRO</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpYKbCBNzY7J6BlubLkTuOZqibtvK9uh1go1Kia1Rb7sxk27wlnuhwR5nQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：7B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2112.04426.pdf</p>
</li>
<li>
<p>发布详情：DeepMind 发布于 2022 年 2 月。</p>
</li>
</ul>
<p>检索是一种通用的技术，即在进行推理时提供一个数据库供其查找。RETRO 是第一篇使用 2T token 数据库的 Transformer 检索论文。它使用预训练的 BERT 式模型将 token 数据库嵌入块中，然后在训练和推理期间对数据库中的最近邻执行分块交叉注意力。</p>
<p><strong>6.GPT-3.5</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpjvWKn0ZJYAzyuxxlNwgpNmiaLBdHj9dBwvnUeo4BMSIosgHg5JZw50g/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：架构未知</p>
</li>
<li>
<p>文档地址 <a href="https://platform.openai.com/docs/guides/gpt">https://platform.openai.com/docs/guides/gpt</a></p>
</li>
<li>
<p>发布详情：OpenAI 发布于 2022 年 3 月。</p>
</li>
</ul>
<p>OpenAI 将三种模型划分为 GTP-3.5，具体包括 davinci-002 系列中的两种和 davinci-003 系列中的一种。其中， code-davinci-002 是基本模型，text-davinci-002 是一个带有 FeedME 非 RL 指令调整的版本。text-davinci-003 是带有 RLHF 的 InstructGPT。有一篇 InstructGPT 论文训练了 RLHF 模型，但没有提到 FeedME，而 text-davinci-002 虽然是 InstructGPT 模型，但没有使用 RLHF。OpenAI API 上的 davinci 模型在 2020 年的论文中被指出是 175B 模型，但从未证实 davinci-002 是否具有相同尺寸。</p>
<p><strong>7.Chinchilla</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpj1SmEiausPgbzypCsyAGgMQZo2fgt9PjvBKtTvN92H523PKiafXGmpyA/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：70B 参数，80 层，8192 嵌入维度，64 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2203.15556.pdf</p>
</li>
<li>
<p>发布详情：DeepMind 发布于 2022 年 3 月。</p>
</li>
</ul>
<p>Chinchilla 的论文中引入了新的、改进版的 scalling law。它使用 1.5T token（与 Gopher 相似的数据集）和与 Gopher 相同的计算量进行训练，但性能优于 Gopher。在 scalling law 中，模型的参数和 token 数按照 20:1 的比例线性增加。学习率采用余弦调度进行调整。Megatron Turing NLG 和 Jurassic J-1 Jumbo 是另外两个大型模型，由于它们不是 Chinchilla 最优模型，也没有独特意义，因此没有在本文中单独记录。</p>
<p><strong>8.Flamingo</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpcd0CMfIgibxUUh50gC0KIyqeibBGJYaR5NibWMyZexlw5jEVt88Bqk3DQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：80B 参数</p>
</li>
<li>
<p>论文地址 <a href="https://arxiv.org/pdf/2204.14198.pdf">https://arxiv.org/pdf/2204.14198.pdf</a></p>
</li>
<li>
<p>发布详情：DeepMind 发布于 2022 年 4 月。</p>
</li>
</ul>
<p>Flamingo 是一个多模态（文本 / 图像）模型。它只生成文本，而图像输入通过视觉编码器（435M 参数）运行，并使用交叉注意力来关注这些输出。它还在视觉编码器之后使用重采样器（194M 参数），无论输入特征的数量如何，都能产生固定（少量）的视觉 token。它们建立在冻结的 Chinchilla 模型上，80B 参数来自添加到 70B Chinchilla 模型中的交叉注意力层。PaLI 是谷歌的图像 / 语言多模态模型。</p>
<p><strong>9.Gato</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpvURyOuFYbHHO8SszXK1GUPkJUwrI15iaU5xuPAgYmY7iaianlfTOoYlfg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：1.18B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2205.06175.pdf</p>
</li>
<li>
<p>发布详情：发布于 2022 年 5 月。</p>
</li>
</ul>
<p>Gato 是一个通用型智能体，算是 Flamingo 的后续产品，但拥有更多的模态。它使用图像和文本，以及按钮按压数据格式化成的 token，还有来自机器人感知的连续数据编码，并尝试使用尽可能少的数据来完成额外的任务。这些任务包括机器人堆叠测试、图像字幕和 Atari。</p>
<p><strong>10.Anthropic LM</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpRRkgIx3uZAtEk64D9ay5icwX8iag5anUvgmgwJlZYOIrDx5VZicNE0h2w/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：52B 参数，64 层，8192 嵌入维度</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2112.00861.pdf</p>
</li>
<li>
<p>发布详情：Anthropic 发布于 2021 年 12 月。</p>
</li>
</ul>
<p>在 400Btoken 上进行训练，但在 Chinchilla 之后的一篇论文（《 Language Models (Mostly) Know What They Know 》）中，Anthropic 使用了为 850B token 训练的具有相同架构的模型。在后来的另一篇关于道德自我纠正的论文中，使用了一个没有明确说明的 175B 模型。</p>
<p><strong>11.PaLM</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpG1yvZJX9ZJAIY9Vl88NdHfVjI1nwgpqkYzibMFWQUg9ibZnxIQoMyrlQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：540B 参数，118 层，18432 嵌入维度，48 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2204.02311.pdf</p>
</li>
<li>
<p>发布详情：Google 发布于 2022 年 4 月。</p>
</li>
</ul>
<p>截至 2023 年 1 月，这是公开已知的最大密集语言模型。PaLM 使用 SwiGLU 激活，使用并行注意力、多查询注意力、旋转嵌入，并对输入和输出嵌入使用相同的矩阵。它没有使用偏置，使用了一个包含 256k 个 token 的 SentencePiece tokenizer。PaLM 是在与 LaMDA 和 GLaM 类似的数据集上，用 780B 个 token 进行训练的。</p>
<p><strong>12.GPT-NeoX</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpO7r8WdLdo8sDb1Y7a1xZNicuNgHAOMMDOyzWqj3eneniaqhzDtVOibMDg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：20B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2204.06745.pdf</p>
</li>
<li>
<p>项目地址：https://github.com/EleutherAI/gpt-neox</p>
</li>
<li>
<p>发布详情：Eleuther AI 发布于 2022 年 2 月。</p>
</li>
</ul>
<p>这是 Eleuther 的一个开源模型。它使用 DeepSpeed (微软) 和 Nvidia Megatron 在 GPU 上进行训练，并使用与 GPT-J 相同的架构修改，在整个 Pile (400B token) 上进行训练。</p>
<p><strong>13.GPT-J</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp7lQ5jjHBxleiclYrnTVhBiaibyiaPT6a2q9ibdYE5nxZJbJW3vv0O0IJ4jQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：6.7B 参数</p>
</li>
<li>
<p>项目地址：https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b</p>
</li>
<li>
<p>发布详情：Eleuther AI 发布于 2021 年 7 月。</p>
</li>
</ul>
<p>GPT-J 因完全开源而闻名，并且与 GPT-3 论文中 6.7B 版本性能相媲美。它在 TPU 上进行训练，并使用旋转嵌入，并行注意力。为降低复杂性，它仅使用了密集注意力层。它是在 Pile 上训练的，Pile 是一个由 Eleuther AI 创建的开放数据集，包含 22 个较小的数据集，包括 Common Crawl、 OpenWebText、书籍和论文。</p>
<p><strong>14.GLaM</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp0CME1j9NWUTM4QuPJ36rhwlwLSYicNlWzpTPzibtT9FESIFwOaHHm6ow/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：1.2T 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2112.06905.pdf</p>
</li>
<li>
<p>发布详情：Google 发布于 2021 年 12 月。</p>
</li>
</ul>
<p>GLaM 被称为「通用语言模型」，是一个混合专家 (MoE) 模型，其中的参数是稀疏激活。它每层有 64 个专家，每个 token 激活 96.6B 参数。每一层都有一个门控单元，它为每个 token 选择 64 个 MLP 中的一个或两个。</p>
<p><strong>15.LAMDA</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp3xA5L0uKlNmIMbj435OgDsPy1Fs1X6aPLTK69w8rk9ajQXKkGeiboqQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：137B 参数，64 层，8192 嵌入维度，128 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2201.08239.pdf</p>
</li>
<li>
<p>发布详情：Google 在 I/O 上演示于 2021 年 5 月，论文发布于 2022 年 1 月。</p>
</li>
</ul>
<p>LaMDA 对话模型是根据 Meena 创建的。它明确有一个包含大量对话 / 论坛的 2.81T 数据集 (用 32k 的 SentencePiece tokenizer 进行编码)。基础模型有时被称为 LaMDA GLM 或 GLM- 137B；LaMDA 在此基础上添加了许多对话微调。</p>
<p>模型训练用了多少个 token 是明确的，它用到了 1024 个 TPUv3，使用率为 56.5%，训练时间为 57.7 天，batch 大小为 256k，可能是 bf16，计算表明这将是 2.81T token 中的约 900B。</p>
<p><strong>16.Switch</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpX3x71tZsic0CrzkXtr7LYLibibACCeQKoibicF4PnrCuKOFrPGXPCa5DGAg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：1T 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2101.03961.pdf</p>
</li>
<li>
<p>发布详情：Google 发布于 2022 年 6 月。</p>
</li>
</ul>
<p>SwitchTransformer 对 GLaM 进行了改进，它只路由到一个专家，从而减少了计算量。它的创新是使用了不同的路由机制，证明了路由到单个专家是有效的。</p>
<p><strong>17.BLOOM</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpDShQlgVdzfJJXDDaD5N8aTHXoHtYMItliaEFAGOiadvgEY1CUxfgOtkA/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：176B 参数，70 层，14336 嵌入维度，112 个注意力头</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2211.05100.pdf</p>
</li>
<li>
<p>发布详情：HuggingFace 发布于 2022 年 7 月。</p>
</li>
</ul>
<p>截止于本文梳理的时间，BLOOM 是最大的开源模型。它在 HuggingFace 语料库 ROOTS 上进行训练，该语料库包含 498 个 HuggingFace 数据集。该模型在 366B token 上进行训练，并且位置编码是用 ALiBi 完成的。它用到了 250k 词汇表大小的 BPE tokenizer，帮助它适应多语言数据。</p>
<p><strong>18.Galactica</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpv0BHZujl2niavj5Eau2kUezP0KNnBcj0JVrqWcIugLONFroMnPVzBhQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：120B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2211.09085.pdf</p>
</li>
<li>
<p>发布详情：Meta 发布于 2022 年 11 月。</p>
</li>
</ul>
<p>Galactica 是一个科学模型，主要以论文、少量代码、其他基于知识的数据和一些 Common Crawl 数据为基础进行预训练。它用 token 对工作记忆进行编码，并使用特殊 token 对引文进行编码。</p>
<p><strong>19.LLaMa</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpRepzXBibKGSe30fSjTdk8nicjS94Bej7GUsv28TjETibBia96icj0C7skoQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：65B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2302.13971.pdf</p>
</li>
<li>
<p>发布详情：Meta 发布于 2023 年 2 月。</p>
</li>
</ul>
<p>LLaMa 像是 Chinchilla 的复制品，有着相当标准的训练组合，大部分为 Common Crawl。</p>
<p><strong>20.OPT</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp5rmxjqg2rusSjrDW107b4PHOhibyMiaQy5EUqewR9HVFib73Ohva1mBbw/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：175B 参数，与 GPT-3 相同的架构</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2205.01068.pdf</p>
</li>
<li>
<p>项目地址：https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf</p>
</li>
<li>
<p>发布详情：Meta 发布于 2022 年 5 月。</p>
</li>
</ul>
<p>这是 GPT-3 的复刻版，它在 Pile 和 PushShift reddit 上训练，只有 180B token。</p>
<p>这些 Meta 论文完全不是相互关联的项目。LLama、OPT 和 Galactica 共有 41 位作者，只有一位是重合的。</p>
<p><strong>21.GLM-130B</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpDHvgmicuc3iasuUVsUhPgCdS8rcjd9RlNtPRVR2U0Zxnr3yVnNxk1ItQ/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>属性：130B 参数</p>
</li>
<li>
<p>论文地址：https://arxiv.org/pdf/2210.02414.pdf</p>
</li>
<li>
<p>发布详情：清华大学发布于 2022 年 10 月。</p>
</li>
</ul>
<p>GLM 是一个开源的双语（中文 / 英文）模型。它使用旋转嵌入和 DeepNorm，并通过 GeGLU 激活 MLP。值得关注的是，它主要以 INT4 进行推理（而其他模型，如 BLOOM 和 OPT，则量化为 INT8）。它还在预训练中加入了 prompt，而不是标准的 GPT 架构，并且使用 GLM 实现了双向注意力。</p>
<p><strong>架构变化</strong></p>
<p><strong>1. 多查询注意力（Multi-Query Attention，MQA）</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp5pY6h9uBGBU1bkib5ia2dqAT8x1f0ICV8HYtia4r1sgQve0TNSr3ZqMZQ/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/1911.02150.pdf</p>
<p>Noam Shazeer 的这篇论文中，key 和 value 在 head 之间共享，大大减少了推理时所需的内存数量，提高了延迟和吞吐量。这是一篇非常简洁的论文，并附有代码和结果。AlphaCode 和 PaLM 都使用 MQA。</p>
<p><strong>2. 稀疏注意力</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpaCXNW2ME0QWoQorXrcA7B7Exeic8icLHMibVIDSib8eZiaah7XzYiatOeLgw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/1904.10509.pdf</p>
<p>在这种机制中，注意力不会应用于所有之前的 token。它描述了稀疏 Transformer 的两种风格，一种是跨步式，即关注最后 N 个 token；另一种是固定式，即关注序列中的部分 token。在 GPT-3 论文中，该模型被描述为交替密集和「局部带状」稀疏层。</p>
<p><strong>3. 混合专家（Mixture-of-Experts，MoE）</strong></p>
<p>关于 MoE 的内容有很多，在介绍 GLaM 和 Switch 时已经提到了一点。因此，此处将罗列一些优秀的原始文献。</p>
<ul>
<li>
<p>2017 年关于 LSTM 的 MoE 论文 <a href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a></p>
</li>
<li>
<p>面向 MoE 的 Deepmind Scaling Laws 论文 <a href="https://arxiv.org/pdf/2202.01169.pdf">https://arxiv.org/pdf/2202.01169.pdf</a></p>
</li>
<li>
<p>训练 1.1T 参数 MoE 的 Meta 论文 ：https://arxiv.org/pdf/2112.10684.pdf</p>
</li>
</ul>
<p>一些谷歌的论文：</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2202.08906.pdf">https://arxiv.org/pdf/2202.08906.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2202.09368.pdf">https://arxiv.org/pdf/2202.09368.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2205.10937.pdf">https://arxiv.org/pdf/2205.10937.pdf</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2202.08906.pdf">https://arxiv.org/pdf/2202.08906.pdf</a></p>
</li>
<li>
<p><a href="https://openreview.net/pdf?id=23ZjUGpjcc">https://openreview.net/pdf?id=23ZjUGpjcc</a></p>
</li>
</ul>
<p><strong>4.FlashAttention</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp0SORSpdqYtY5X0sic4wFicT8rSMZVYrVCicPC1n3H5hZC9E4Kq7hf72KQ/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2205.14135.pdf</p>
<p>FlashAttention 是一种架构变革，能以更少的内存访问量完成注意力处理。它对注意力矩阵进行切片和增量化的 softmax 约简，并避免了在后向传播过程中存储整个中间注意力矩阵。论文指出，与 megatron 相比，它训练速度提高到 1.7 倍，推理速度提高到 4 倍多（上下文长度越长，倍数越大）。在此之前，另一篇文章 (<a href="https://arxiv.org/pdf/2112.05682.pdf">https://arxiv.org/pdf/2112.05682.pdf</a>) 也在 TPU 上采用了同样的方法，实现了 O (log_n) 内存占用。</p>
<p><strong>5. 编码器 + 解码器</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp3baarEA95h9uPDKCuSjZjDb1RBMO1d1mdQYEKYTfXzOF3h42XVgibVA/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/1706.03762.pdf</p>
<p>根据 Transformer 的原始论文，编码器 - 解码器架构最初是为翻译任务而设计的。经典的 GPT 架构交替使用注意力和 MLP 模块。原始的 Transformer 则采用了编码器块和解码器块。编码器块的结构是：注意力机制 → MLP；解码器块的结构是：掩蔽注意力→ 编码器 - 解码器注意力 → MLP。对于许多序列到序列的任务来说，例如 AlphaCode 或 T5，这也是一个合理的架构。</p>
<p><strong>6. 平行注意力</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpWroytj8Vff2iaYHxurukiaWw8ywYNJschQn8OS3hBxbpDbEN8ZJp1AWw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2204.02311.pdf</p>
<p>PaLM 使用平行注意力。即在训练模型时，注意力层和 MLP 层并行运行，使用相同的向量。如此一来，就可以将注意力和前馈矩阵乘法合并在一起，从而提升运算强度，获得更好的性能（PaLM 的训练速度提升了 15%）。GPT-J 也使用了这种方法。</p>
<p><strong>7. 可供选择的激活方案：GeGLU，SwiGLU，SoLU</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpAJSS73szYy3kjsengibFSTTe4jUXZz4JNITHXT81jcFS4wmKG5VduSw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/1706.03762.pdf</p>
<p>最初的 Transformer 论文使用 ReLU 来激活 MLP 模块。它在两个线性变换（matmuls）之间进行简单的 x if &gt; x = 0 else 0。从直观上看，这有点草率。GeLU 与 ReLU 类似，但要平滑一些。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpDdrXs2dpWHz04NcynBlS5wsef5gWgYB3XkZvPRXlsdJKWv28gf1Szg/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://transformer-circuits.pub/2022/solu/index.html</p>
<p>SoLU（Softmax）简单地说就是 x*softmax (x)，用于提高模型的可解释性。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpDbZeOe1zbL7gnQYNMmoRouhK9VZ5ygPu7JibWuDxTfoib7v1LIguGajw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2002.05202.pdf</p>
<p>SwiGLU 是所列论文中最复杂的，也是 Noam Shazee 的个人论文。它建立在门控线性单元的基础上，旨在比 ReLU 更稳定，并在 GLU 之前进行 swish 运算。与 GeLU 一样，它软化了 ReLU，允许某些值低于零。</p>
<p><strong>8.LayerNorm 的替代方案：DeepNorm，RMSNorm</strong></p>
<p>LLM 每个区块有两次 norm（一次用于注意力，一次用于前馈），它会执行一些归一化功能以改进训练。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpdibibCstJ9El9l4eAI9ONTbYXvA7pvvro6kW0UwyxyDwRctsWlAOGNmQ/640?wx_fmt=png" alt=""></p>
<p>DeepNorm 论文地址：https://arxiv.org/pdf/2203.00555.pdf)</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpMuRUPwJOBlzRJf18dZ3ADatwCLyNx6eTxJ5rxVdezKvjJmF3dfF6sA/640?wx_fmt=png" alt=""></p>
<p>RMSNorm 论文地址：https://arxiv.org/pdf/1910.07467.pdf</p>
<p>DeepNorm 和 RMSNorm 可以成为替代方案。RMSNorm（均方根）简单来说就是数值均值的平方根。还有一种 batch norm，效率很低，用起来似乎不太聪明。</p>
<p><strong>9.RoPE</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp7Swdfn2ia0BBzLS9bwISlOXwibL0mHlk3UAoLm5pWmLvuauibsIBXsIXg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>论文地址：https://arxiv.org/pdf/2104.09864.pdf</p>
</li>
<li>
<p>相关 Blog 文章：https://blog.eleuther.ai/rotary-embeddings/</p>
</li>
</ul>
<p>这篇 Blog 文章总结得十分优秀，本文不做赘述。</p>
<p><strong>10.BPE vs SentencePiece Tokenizers</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp7Swdfn2ia0BBzLS9bwISlOXwibL0mHlk3UAoLm5pWmLvuauibsIBXsIXg/640?wx_fmt=png" alt=""></p>
<ul>
<li>
<p>BPE 项目地址：https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt</p>
</li>
<li>
<p>SentencePiece 编码器项目地址：https://github.com/google/sentencepiece</p>
</li>
</ul>
<p>字节对编码（Byte Pair Encoding，BPE）是大多数语言模型的默认编码，最初的 GPT 论文、GPT-3 以及 GPT-3.5 都使用了这种编码。不使用纯 BPE，而使用 SentencePiece 情况的一个明显原因是，分布不包含空格分隔的单词，就像 AlphaCode、GLM（中文）和 PaLM（明确是因为多语言）那样。</p>
<p><strong>11.ALiBi</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpZaFWGOOPGLujgUSLVESF1tSddJujQokibcnZcORc6nGIkhE9Iqq7OtA/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2108.12409.pdf</p>
<p>ALiBi（Attention with Linear Biases）是一种长上下文位置嵌入方案，通过根据距离对 qk 分数进行线性偏置，来支持对更长的长度进行外推。BLOOM 用了 ALiBi，Galactica 也尝试过，但没有采用。</p>
<p><strong>预训练后处理技术</strong></p>
<p><strong>1. 采用 PPO 算法的 RLHF</strong></p>
<p>在 RLHF 中，首先要训练一个奖励模型，由标注员评估模型生成的数组。然后在 RL 中使用 PPO（近端策略优化），策略生成由奖励模型评估的输出，以改进策略。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpMglflXqug14hjibE6MAVlGZhicbv0OVgUxUtibqHPKo8z9aIs4SQKS5wA/640?wx_fmt=png" alt=""></p>
<p>Christiano 论文：https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</p>
<p>Deepmind 的 Sparrow 和 Anthropic 的 LM 都是用 RL (AI|H) F 训练的，它们都有对话界面。WebGPT 和 GopherCite 一样，也是用 RLHF 训练的（后者调用了 RLHPreferences）。我认为，这都起源于 2017 年的 Christiano，它先于 LLM 所有内容，之后才是 2020 年根据人类反馈进行的总结以及 PPO 论文。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpEjqmlBPXE9cxaItk2WoYM6x5xagmsORmAm8v85zhViaLuantkwEXc1Q/640?wx_fmt=png" alt=""></p>
<p>2020 年根据人类反馈进行的总结 <a href="https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf</a></p>
<p><strong>2.Constitutional</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpjZUJTBweDrrA8SNCRbWnQhwtpVyr1bB3INM278ibeyDVgfvYUVFgqNw/640?wx_fmt=png" alt=""></p>
<p>论文链接：https://arxiv.org/pdf/2212.08073.pdf</p>
<p>作为 RLHF 的扩展，Constitutional 基本上是 RLAIF，不过实际上被称为 CAI。它有一个监督学习阶段，在这一阶段，只提供帮助的 AI 会生成对抗性 prompt。然后，助手会根据所提供的 constitution（以字符串的形式提供给模型的一组短值）迭代出自己的响应。然后对这些响应进行微调。第二阶段就像采用 PPO 的 RLHF，只不过将 AI 反馈替换了。</p>
<p><strong>3.Minerva</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpLOIGMDQicnkWqNXa21CF1qrxyoQHqlCiaEIRIV5RpdP29u35C309lc5Q/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2206.14858.pdf</p>
<p>Minerva 是 Blueshift 团队于 2022 年 6 月发布的一个数学和科学数据微调模型，执行效果非常好。它是一个来自 PaLM 的 62/540B 微调模型。它的数据集来自 ArXiV 和一些网站，并经过精心预处理，保留了数学格式。</p>
<p><strong>4.Codex</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp9w7LEibeQtRvZ4nhwtzibdnbOHRfkUiciay7sF3qrVblnia2zs8Kicul3GBg/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2107.03374.pdf</p>
<p>Codex 于 2021 年 7 月推出（并支撑了 Github Copilot 的推出)，是在 100B token 代码 (此处为公开的 Github 代码) 上微调而成的。该论文还首次提出了 HumanEval，即人类编写的代码评估。本文最值得注意的是，它证明了代码数据对代码性能非常重要，因为 GPT-J 在代码方面的表现优于 GPT-3。他们还为代码添加了一些 token，这使压缩率提高了 30%。</p>
<p><strong>5. 只对 CoTed 输出进行微调</strong></p>
<p>我忘了哪篇论文是这么做的，但依稀记得他们根据模型的思维链输出对模型进行了微调，结果变得更好。虽然这在意料之中，但是也值得关注。</p>
<p><strong>6.FeedME (SFT)</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpWaALyKkpUI1Jpu6YzP3ULpcxueoKV8lKzDicHBPDzk3ia9cjibqC8GLRQ/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2203.02155.pdf</p>
<p>这种方法在 Instruct GPT 论文中有所描述，但这不一定是该方法起源。该方法的起源更加接近下面这篇论文。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpAEAbaPKBcacWe5k0n5mBy9kWuBa7ZXicXyYTaRJnPtWnnRRgz74Ak1w/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/1909.08593.pdf</p>
<p>监督微调使用人工生成的内容，然后用于微调预训练模型。论文发现，SFT 比基础预训练模型表现更好，但 RLHF 比 SFT 表现更好。</p>
<p><strong>7.FLAN</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpeyVnaQZNhF6u4jo9ic8DKKH10CDef7zUN02Z6D8o4DLwVGeZibXnZicgQ/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2109.01652.pdf</p>
<p>FLAN 是一个经过指令调整的模型（在指令格式的 nlp 任务上进行了微调），可提升零样本性能。</p>
<p><strong>训练技术</strong></p>
<p><strong>1. 善于设置超参数</strong></p>
<p>没有论文是专门讨论这个的，但正确设置超参数显然是非常重要的。</p>
<p>通过阅读以下文章可以获得一些基准。</p>
<ul>
<li>
<p>Chinchilla 论文：https://arxiv.org/pdf/2203.15556.pdf</p>
</li>
<li>
<p>Scalling Laws 论文 <a href="https://arxiv.org/pdf/2001.08361.pdf">https://arxiv.org/pdf/2001.08361.pdf</a></p>
</li>
<li>
<p>Jane Street 的有关理解批大小的博客文章：https://blog.janestreet.com/does-batch-size-matter/</p>
</li>
</ul>
<p><strong>2. 基于人类反馈的预训练</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpRgJOFQ8g9Hfr7VDGicMmBNNBgmGV4fBltKfVrz7QiaI61L2e2pmL40ew/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2302.08582.pdf</p>
<p>尽管 PHF（Pretraining with Human Feedback）在预训练时使用了一种简单的技术来标记数据，但预训练往往采用无监督的形式。该方法在训练时使用两个条件 token（好的和坏的）预置到样本中，然后在推理时使用它们进行采样。该研究还尝试了其他各种目标（尤其是把坏数据过滤掉），但在 python 风格、PII 和毒性上的评估结果都很差。</p>
<p><strong>3.MuP</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLphNYXKSTUtL48fmZ8vA2stntT5LVEicXufen8iaNqiaScW4wuibjTuVbzYA/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2203.03466.pdf</p>
<p>MuP（Maximal Update Parameterization ）是一种参数化方法，这种方法不仅节省了参数扫描计算，而且更接近最优。这篇论文很好地阐述了这一方法的理论依据。</p>
<p><strong>其他</strong></p>
<p><strong>1. 思维链（CoT）</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLphib0vXgelrPH0GacMzBOicmdgJP8ltNxpBxEhfiaFU6nglFQN5fjyWcrg/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2201.11903.pdf</p>
<p>CoT 是一种让模型 「step-by-step」思考并产生更好结果的技术，名字起源于上述论文《 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models 》。论文描述了发表于 2021 年 2 月的论文《Prompt Programming for Large Language Models:Beyond the Few-Shot Paradigm》中技术的具体应用。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpgtaCwBIwkjujhtSlq4dgLtsD2HLr94j8dfTSia26OJJickFCr8BPowNw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2102.07350.pdf</p>
<p><strong>2. 工具使用</strong></p>
<p>关于规范工具使用的论文可以最早追溯到 2021 年 12 月的 WebGPT 论文。文中 GPT-3 可以访问网络，从而大大增强了模型能力。</p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLp5FqlkhynPxiaevBBvTP8QPU7uNsIABb8N5leYmpHwVwjNNFlpV3iapcg/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2112.09332.pdf</p>
<p>除此以外，DeepMind 还训练了可以借助 RL 工具来完成各种任务的智能体 ；Meta 发布语言模型 Toolformer，可以教会自己使用工具。</p>
<ul>
<li>
<p>DeepMind 论文：https://arxiv.org/pdf/2202.08137.pdf</p>
</li>
<li>
<p>Meta 的 Toolformer：https://arxiv.org/pdf/2302.04761.pdf</p>
</li>
</ul>
<p><strong>3.Fill In the Middle</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLptejmaJxFiaXKbf6wrQCfNBGVXiaicmTOjAZvJKYAyVtKTuiaKYS6ibYt1Lw/640?wx_fmt=png" alt=""></p>
<p>论文地址：https://arxiv.org/pdf/2207.14255.pdf</p>
<p>这篇论文描述了一种简单的数据转换，它将子字符串从文本中间移到末尾，并要求模型填充中间部分。这样，模型就能获得一种对代码补全等任务非常有用的能力，而不会影响严格意义上从左到右任务的性能。</p>
<p><strong>4. 采样技术：Top-k，Top-p (核)，Beam Search</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpzddwkag38L3PTCvTtnA5XGwLenrZ5w4qdkGKLq7OlwoQg2vNSibhLYA/640?wx_fmt=png" alt=""></p>
<p>与 Top -P 有关的论文地址：https://arxiv.org/pdf/1904.09751.pdf</p>
<p>语言模型的输出基本上是每个可能 token 的 logit，然后将其 softmax 化为概率。将 logits 转换为 token 的最简单方法，就是取最有可能的 token。当语言模型有温度控制时，它将 logits 除以温度，这使模型对其首选更有信心 / 更没有信心。Top -K 采样从该分布中获取前 K 个 token 和样本。Top -P 采样，或称核采样，会选择 tokens 中概率累积排名前 P 个百分比的部分，并从这个选定的部分进行抽样。</p>
<p><strong>5. 无尾采样（Tail Free Sampling）</strong></p>
<p><img src="https://api.allorigins.win/raw?url=https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWibwK2TxMMcXEnEt9e6wOtLpOVT4qwmKnUtnsmkc2I11oRJLURubiazUvI8HK1l0ibzz2AdISyQqWMXA/640?wx_fmt=png" alt=""></p>
<p>文章地址：https://www.trentonbricken.com/Tail-Free-Sampling/</p>
<p>无尾采样是 Top-p 采样的衍生，之所以这样命名是为了找到 「尾」，因为 Top-p 采样可能会在许多 token 具有相似概率的点上被切断而失败。上面这篇文章像是说明了无尾采样能够更好进行采样工作的原因，但当涉及到提高模型的创造力和范围时，没有很好的基准。</p>
<p>补充地址（文章中提到的其他论文的地址）如下：</p>
<ul>
<li>
<p>GPT-2 论文（2018）：https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</p>
</li>
<li>
<p>扩展定律论文：https://arxiv.org/pdf/2001.08361.pdf</p>
</li>
<li>
<p>Transformer 论文 2017：https://arxiv.org/pdf/1706.03762.pdf</p>
</li>
<li>
<p>Turing NLG 论文：https://arxiv.org/pdf/2201.11990.pdf</p>
</li>
<li>
<p>Jurassic J-1 Jumbo 论文：https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf</p>
</li>
<li>
<p>PaLI 论文：https://arxiv.org/pdf/2209.06794.pdf</p>
</li>
<li>
<p>post-Chinchilla 论文：https://arxiv.org/pdf/2207.05221.pdf</p>
</li>
<li>
<p>有关道德自我纠正的论文：https://arxiv.org/pdf/2302.07459.pdf</p>
</li>
<li>
<p>近端策略优化论文：https://arxiv.org/pdf/1707.06347.pdf</p>
</li>
<li>
<p>Deepmind 的 Sparrow 论文：https://arxiv.org/pdf/2209.14375.pdf</p>
</li>
<li>
<p>WebGPT 论文：https://arxiv.org/pdf/2112.09332.pdf</p>
</li>
<li>
<p>GopherCite 论文：https://arxiv.org/pdf/2203.11147.pdf</p>
</li>
</ul>
<p><em>原文链接：https://kipp.ly/transformer-taxonomy/?continueFlag=a897a8d0eb16dcae5398f1b58cc5e06f</em></p>
<p>更多AI工具，参考<a href="https://ai123.869hr.uk/">Github-AI123</a>，<a href="https://ai123.869hr.uk/">国内AI123</a></p>



          </div>

<<<<<<< HEAD

=======
 可扫如下微信二维码加好友
>>>>>>> HEAD@{1}

<p><img src="/images/aitools/2024/03/qrcode_for_gh_dde1b429630d_258.jpg" alt=""></p>

        </article>

      </div>
    </div>
  </div>
</section>
        </div>
    </div>
    </main>




<script type='text/javascript' src='/assets/js/jquery.ui.touch-punch.min-0.2.2.js' id='jqueryui-touch-js'></script>
<script type='text/javascript' src='/assets/js/clipboard.min-5.6.2.js' id='clipboard-js'></script>
<script type='text/javascript' src='/assets/js/tooltip-extend.js' id='iplaycode-nav-js'></script>
<script type='text/javascript' id='popper-js-extra'>
 

var theme = {"ajaxurl":"","addico":"https:\/\/nav.baidu.cn\/wp-content\/themes\/onenav\/images\/add.png","order":"asc","formpostion":"top","defaultclass":"io-grey-mode","isCustomize":"1","icourl":"","icopng":".png","urlformat":"1","customizemax":"10","newWindow":"0","lazyload":"1","minNav":"1","loading":"1","hotWords":"baidu","classColumns":" col-sm-6 col-md-4 col-xl-5a col-xxl-6a ","apikey":"TWpBeU1UVTNOekk1TWpVMEIvZ1M2bFVIQllUMmxsV1dZelkxQTVPVzB3UW04eldGQmxhM3BNWW14bVNtWk4="};
 
</script>
<script type='text/javascript' src='/assets/js/popper.min.js' id='popper-js'></script>
<script type='text/javascript' src='/assets/js/bootstrap.min-4.3.1.js' id='bootstrap-js'></script>
<script type='text/javascript' src='/assets/js/theia-sticky-sidebar-1.5.0.js' id='sidebar-js'></script>
<script type='text/javascript' src='/assets/js/lazyload.min-12.4.0.js' id='lazyload-js'></script>
<script type='text/javascript' src='/assets/js/fancybox.min-3.5.7.js' id='lightbox-js-js'></script>

<script type='text/javascript' src='/assets/js/app-anim.js' id='appanim-js'></script>

<script type="text/javascript">
    $(document).ready(function(){
        var siteWelcome = $('#loading');
        siteWelcome.addClass('close');
        setTimeout(function() {
            siteWelcome.remove();
        }, 600);
    });
</script>
<script>        
    $(document).ready(function(){
        setTimeout(function () {
            if ($('a.smooth[href="' + window.location.hash + '"]')[0]) {
                $('a.smooth[href="' + window.location.hash + '"]').click();
            }else if (window.location.hash != '') {
                $("html, body").animate({
                    scrollTop: $(window.location.hash).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
        }, 300);
        $(document).on('click','a.smooth',function(ev) {
            if($('#sidebar').hasClass('show') && !$(this).hasClass('change-href')){
                $('#sidebar').modal('toggle');
            }
            if($(this).attr("href").substr(0, 1) == "#"){
                $("html, body").animate({
                    scrollTop: $($(this).attr("href")).offset().top - 90
                }, {
                    duration: 500,
                    easing: "swing"
                });
            }
            if($(this).hasClass('go-search-btn')){
                $('#search-text').focus();
            }
            if(!$(this).hasClass('change-href')){
                var menu =  $("a"+$(this).attr("href"));
                menu.click();
                toTarget(menu.parent().parent(),true,true);
            }
        });
        $(document).on('click','a.tab-noajax',function(ev) {
            var url = $(this).data('link');
            if(url)
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').show().attr('href', url);
            else
                $(this).parents('.d-flex.flex-fill.flex-tab').children('.btn-move.tab-move').hide();
        });
        
    });
</script>

<script>

(function(){
    if(document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") === ''){
        if(new Date().getHours() > 22 || new Date().getHours() < 6){
            document.body.classList.remove('io-black-mode');
            document.body.classList.add('io-grey-mode');
            document.cookie = "night=1;path=/";
            console.log('夜间模式开启');
        }else{
            document.body.classList.remove('night');
            document.cookie = "night=0;path=/";
            console.log('夜间模式关闭');
        }
    }else{
        var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
        if(night == '0'){
            document.body.classList.remove('night');
        }else if(night == '1'){
            document.body.classList.add('night');
        }
    }
})();

$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");   
function switchNightMode(){
    var night = document.cookie.replace(/(?:(?:^|.*;\s*)night\s*\=\s*([^;]*).*$)|^.*$/, "$1") || '0';
    if(night == '0'){
	$("#search-bg").css("background", "linear-gradient(#e2c4c4, #d8d8d8)");
        document.body.classList.remove('io-grey-mode');
        document.body.classList.add('io-black-mode');
        document.cookie = "night=1;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","日间模式");
        $(".mode-ico").removeClass("icon-night");
        $(".mode-ico").addClass("icon-light");
    }else{
	$("#search-bg").css("background", "linear-gradient(#4f4040, #1b1d1f)");
        document.body.classList.remove('io-black-mode');
        document.body.classList.add('io-grey-mode');
        document.cookie = "night=0;path=/"
        console.log(' ');
        $(".switch-dark-mode").attr("data-original-title","夜间模式");
        $(".mode-ico").removeClass("icon-light");
        $(".mode-ico").addClass("icon-night");
    }
}
</script>


<script>
    var newsContainer = document.getElementById('news-container');
    var newsItems = document.getElementsByClassName('news-item');
    var currentItem = 0;

    setInterval(function() {
        
        newsItems[currentItem].classList.remove('show');
        newsItems[currentItem].style.transform = 'translateY(-20px)';
        
        currentItem = (currentItem + 1) % newsItems.length;
        newsItems[currentItem].style.transform = 'translateY(' + (newsContainer.offsetHeight - 20) + 'px)';
        setTimeout(function() {
            newsItems[currentItem].classList.add('show');
        }, 500);
    }, 8000);
</script>

</body>
</html>


